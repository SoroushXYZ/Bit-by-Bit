{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deduplication Algorithms Testing\n",
        "\n",
        "This notebook tests different approaches to remove duplicate articles from our RSS feed data.\n",
        "\n",
        "## Approaches to Test\n",
        "\n",
        "1. **Custom Implementation** - URL matching, title similarity, content similarity, fuzzy matching\n",
        "2. **all-MiniLM-L6-v2** - Sentence transformer embeddings for semantic similarity\n",
        "\n",
        "## Goals\n",
        "\n",
        "- Compare effectiveness of different approaches\n",
        "- Measure performance (time taken, duplicates found)\n",
        "- Save duplicate pairs for manual validation\n",
        "- Find the best approach for our use case\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries imported successfully\n",
            "ğŸ“¦ Sentence Transformers available: True\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from difflib import SequenceMatcher\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# For sentence transformers\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SENTENCE_TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"âš ï¸ sentence-transformers not available. Install with: pip install sentence-transformers scikit-learn\")\n",
        "\n",
        "print(\"âœ… Libraries imported successfully\")\n",
        "print(f\"ğŸ“¦ Sentence Transformers available: {SENTENCE_TRANSFORMERS_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“‚ Loading RSS data...\n",
            "âœ… Loaded 255 articles\n",
            "ğŸ“Š DataFrame shape: (255, 12)\n",
            "ğŸ“ Columns: ['title', 'url', 'summary', 'content', 'published', 'author', 'feed_name', 'feed_category', 'feed_url', 'tags', 'guid', 'raw_entry']\n",
            "\n",
            "ğŸ“‹ Sample article:\n",
            "   title: Find out whatâ€™s new in the Gemini app in Septemberâ€™s Gemini Drop....\n",
            "   url: https://blog.google/products/gemini/gemini-drop-september-2025/...\n",
            "   feed_name: Google The Keyword...\n",
            "   published: 2025-09-19T16:00:00...\n"
          ]
        }
      ],
      "source": [
        "# Load RSS data\n",
        "print(\"ğŸ“‚ Loading RSS data...\")\n",
        "with open('../data/rss/rss_data.json', 'r') as f:\n",
        "    rss_data = json.load(f)\n",
        "\n",
        "articles = rss_data['articles']\n",
        "print(f\"âœ… Loaded {len(articles)} articles\")\n",
        "\n",
        "# Convert to DataFrame for easier processing\n",
        "df = pd.DataFrame(articles)\n",
        "print(f\"ğŸ“Š DataFrame shape: {df.shape}\")\n",
        "print(f\"ğŸ“ Columns: {list(df.columns)}\")\n",
        "\n",
        "# Show sample data\n",
        "print(\"\\nğŸ“‹ Sample article:\")\n",
        "sample = df.iloc[0]\n",
        "for col in ['title', 'url', 'feed_name', 'published']:\n",
        "    if col in sample:\n",
        "        print(f\"   {col}: {sample[col][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for text processing\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text for comparison\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove common punctuation that might vary\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def normalize_url(url):\n",
        "    \"\"\"Normalize URL for comparison\"\"\"\n",
        "    if not url or pd.isna(url):\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        # Remove www, trailing slashes, fragments\n",
        "        domain = parsed.netloc.replace('www.', '')\n",
        "        path = parsed.path.rstrip('/')\n",
        "        return f\"{parsed.scheme}://{domain}{path}\"\n",
        "    except:\n",
        "        return str(url).lower()\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"Calculate similarity between two texts using SequenceMatcher\"\"\"\n",
        "    if not text1 or not text2:\n",
        "        return 0.0\n",
        "    return SequenceMatcher(None, text1, text2).ratio()\n",
        "\n",
        "print(\"âœ… Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” APPROACH 1: Custom Implementation\n",
            "==================================================\n",
            "â±ï¸  Processing time: 12.98 seconds\n",
            "ğŸ” Total duplicates found: 5\n",
            "ğŸ“Š Duplicates by type:\n",
            "   exact_url: 4\n",
            "   similar_title: 1\n",
            "ğŸ’¾ Results saved to ../data/deduplication/custom_implementation_results.json\n",
            "âœ… Custom implementation completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 1: Custom Implementation\n",
        "print(\"ğŸ” APPROACH 1: Custom Implementation\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare data\n",
        "df['clean_title'] = df['title'].apply(clean_text)\n",
        "df['clean_content'] = df['content'].apply(clean_text)\n",
        "df['normalized_url'] = df['url'].apply(normalize_url)\n",
        "\n",
        "# Find duplicates using multiple criteria\n",
        "duplicates = []\n",
        "seen_urls = set()\n",
        "seen_titles = set()\n",
        "seen_content = set()\n",
        "\n",
        "# Group by normalized URL first (exact URL matches)\n",
        "url_groups = df.groupby('normalized_url')\n",
        "for url, group in url_groups:\n",
        "    if len(group) > 1:\n",
        "        for i, row1 in group.iterrows():\n",
        "            for j, row2 in group.iterrows():\n",
        "                if i < j:  # Avoid duplicate pairs\n",
        "                    duplicates.append({\n",
        "                        'type': 'exact_url',\n",
        "                        'article1': {'index': i, 'title': row1['title'], 'url': row1['url'], 'feed': row1['feed_name']},\n",
        "                        'article2': {'index': j, 'title': row2['title'], 'url': row2['url'], 'feed': row2['feed_name']},\n",
        "                        'similarity': 1.0\n",
        "                    })\n",
        "\n",
        "# Find similar titles (fuzzy matching)\n",
        "for i in range(len(df)):\n",
        "    for j in range(i + 1, len(df)):\n",
        "        row1, row2 = df.iloc[i], df.iloc[j]\n",
        "        \n",
        "        # Skip if already found as URL duplicate\n",
        "        if (i, j) in [(d['article1']['index'], d['article2']['index']) for d in duplicates]:\n",
        "            continue\n",
        "            \n",
        "        title_sim = calculate_similarity(row1['clean_title'], row2['clean_title'])\n",
        "        \n",
        "        if title_sim > 0.8:  # High title similarity\n",
        "            duplicates.append({\n",
        "                'type': 'similar_title',\n",
        "                'article1': {'index': i, 'title': row1['title'], 'url': row1['url'], 'feed': row1['feed_name']},\n",
        "                'article2': {'index': j, 'title': row2['title'], 'url': row2['url'], 'feed': row2['feed_name']},\n",
        "                'similarity': title_sim\n",
        "            })\n",
        "\n",
        "# Find similar content (for articles with similar titles)\n",
        "for i in range(len(df)):\n",
        "    for j in range(i + 1, len(df)):\n",
        "        row1, row2 = df.iloc[i], df.iloc[j]\n",
        "        \n",
        "        # Skip if already found\n",
        "        if (i, j) in [(d['article1']['index'], d['article2']['index']) for d in duplicates]:\n",
        "            continue\n",
        "            \n",
        "        # Only check content if titles are somewhat similar\n",
        "        title_sim = calculate_similarity(row1['clean_title'], row2['clean_title'])\n",
        "        if title_sim > 0.3:  # Some title similarity\n",
        "            content_sim = calculate_similarity(row1['clean_content'], row2['clean_content'])\n",
        "            \n",
        "            if content_sim > 0.7:  # High content similarity\n",
        "                duplicates.append({\n",
        "                    'type': 'similar_content',\n",
        "                    'article1': {'index': i, 'title': row1['title'], 'url': row1['url'], 'feed': row1['feed_name']},\n",
        "                    'article2': {'index': j, 'title': row2['title'], 'url': row2['url'], 'feed': row2['feed_name']},\n",
        "                    'similarity': content_sim,\n",
        "                    'title_similarity': title_sim\n",
        "                })\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "# Results\n",
        "print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
        "print(f\"ğŸ” Total duplicates found: {len(duplicates)}\")\n",
        "\n",
        "# Count by type\n",
        "type_counts = defaultdict(int)\n",
        "for dup in duplicates:\n",
        "    type_counts[dup['type']] += 1\n",
        "\n",
        "print(f\"ğŸ“Š Duplicates by type:\")\n",
        "for dup_type, count in type_counts.items():\n",
        "    print(f\"   {dup_type}: {count}\")\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'approach': 'custom_implementation',\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'processing_time_seconds': processing_time,\n",
        "    'total_duplicates': len(duplicates),\n",
        "    'duplicates_by_type': dict(type_counts),\n",
        "    'duplicate_pairs': duplicates\n",
        "}\n",
        "\n",
        "os.makedirs('../data/deduplication', exist_ok=True)\n",
        "with open('../data/deduplication/custom_implementation_results.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"ğŸ’¾ Results saved to ../data/deduplication/custom_implementation_results.json\")\n",
        "print(\"âœ… Custom implementation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” APPROACH 2: all-MiniLM-L6-v2 Sentence Transformers\n",
            "==================================================\n",
            "ğŸ“¥ Loading all-MiniLM-L6-v2 model...\n",
            "ğŸ”„ Preparing text for embedding...\n",
            "ğŸ§  Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0507ac4c9e64b3f9a5a26dfcf1da8ca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š Calculating similarity matrix...\n",
            "ğŸ” Finding duplicates with similarity > 0.5...\n",
            "â±ï¸  Processing time: 4.55 seconds\n",
            "ğŸ” Total duplicates found: 119\n",
            "ğŸ“Š Similarity threshold used: 0.5\n",
            "\n",
            "ğŸ“‹ Sample duplicates found:\n",
            "   1. Similarity: 0.885\n",
            "      Article 1: Chrome: The browser you love, reimagined with AI...\n",
            "      Article 2: Go behind the browser with Chromeâ€™s new AI features...\n",
            "\n",
            "   2. Similarity: 0.762\n",
            "      Article 1: Chrome: The browser you love, reimagined with AI...\n",
            "      Article 2: Google announces massive expansion of AI features in Chrome...\n",
            "\n",
            "   3. Similarity: 0.863\n",
            "      Article 1: Chrome: The browser you love, reimagined with AI...\n",
            "      Article 2: Google stuffs Chrome full of AI features whether you like it...\n",
            "\n",
            "ğŸ’¾ Results saved to ../data/deduplication/sentence_transformers_results.json\n",
            "âœ… Sentence transformers approach completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 2: all-MiniLM-L6-v2 Sentence Transformers\n",
        "print(\"\\nğŸ” APPROACH 2: all-MiniLM-L6-v2 Sentence Transformers\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not SENTENCE_TRANSFORMERS_AVAILABLE:\n",
        "    print(\"âŒ Sentence transformers not available. Skipping this approach.\")\n",
        "    print(\"   Install with: pip install sentence-transformers scikit-learn\")\n",
        "else:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load the model\n",
        "    print(\"ğŸ“¥ Loading all-MiniLM-L6-v2 model...\")\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    \n",
        "    # Prepare text for embedding (combine title and content)\n",
        "    print(\"ğŸ”„ Preparing text for embedding...\")\n",
        "    texts = []\n",
        "    for _, row in df.iterrows():\n",
        "        title = str(row['title']) if pd.notna(row['title']) else \"\"\n",
        "        content = str(row['content']) if pd.notna(row['content']) else \"\"\n",
        "        # Combine title and content with a separator\n",
        "        combined_text = f\"{title} [SEP] {content}\"\n",
        "        texts.append(combined_text)\n",
        "    \n",
        "    # Generate embeddings\n",
        "    print(\"ğŸ§  Generating embeddings...\")\n",
        "    embeddings = model.encode(texts, show_progress_bar=True)\n",
        "    \n",
        "    # Calculate similarity matrix\n",
        "    print(\"ğŸ“Š Calculating similarity matrix...\")\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    \n",
        "    # Find duplicates based on similarity threshold\n",
        "    threshold = 0.6  # Adjust this threshold as needed\n",
        "    duplicates_st = []\n",
        "    \n",
        "    print(f\"ğŸ” Finding duplicates with similarity > {threshold}...\")\n",
        "    for i in range(len(similarity_matrix)):\n",
        "        for j in range(i + 1, len(similarity_matrix)):\n",
        "            similarity = similarity_matrix[i][j]\n",
        "            \n",
        "            if similarity > threshold:\n",
        "                row1, row2 = df.iloc[i], df.iloc[j]\n",
        "                duplicates_st.append({\n",
        "                    'type': 'semantic_similarity',\n",
        "                    'article1': {'index': i, 'title': row1['title'], 'url': row1['url'], 'feed': row1['feed_name']},\n",
        "                    'article2': {'index': j, 'title': row2['title'], 'url': row2['url'], 'feed': row2['feed_name']},\n",
        "                    'similarity': float(similarity)\n",
        "                })\n",
        "    \n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    \n",
        "    # Results\n",
        "    print(f\"â±ï¸  Processing time: {processing_time:.2f} seconds\")\n",
        "    print(f\"ğŸ” Total duplicates found: {len(duplicates_st)}\")\n",
        "    print(f\"ğŸ“Š Similarity threshold used: {threshold}\")\n",
        "    \n",
        "    # Show some examples\n",
        "    print(f\"\\nğŸ“‹ Sample duplicates found:\")\n",
        "    for i, dup in enumerate(duplicates_st[:3]):\n",
        "        print(f\"   {i+1}. Similarity: {dup['similarity']:.3f}\")\n",
        "        print(f\"      Article 1: {dup['article1']['title'][:60]}...\")\n",
        "        print(f\"      Article 2: {dup['article2']['title'][:60]}...\")\n",
        "        print()\n",
        "    \n",
        "    # Save results\n",
        "    results_st = {\n",
        "        'approach': 'all_minilm_l6_v2',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'processing_time_seconds': processing_time,\n",
        "        'total_duplicates': len(duplicates_st),\n",
        "        'similarity_threshold': threshold,\n",
        "        'model_name': 'all-MiniLM-L6-v2',\n",
        "        'duplicate_pairs': duplicates_st\n",
        "    }\n",
        "    \n",
        "    with open('../data/deduplication/sentence_transformers_results.json', 'w') as f:\n",
        "        json.dump(results_st, f, indent=2)\n",
        "    \n",
        "    print(f\"ğŸ’¾ Results saved to ../data/deduplication/sentence_transformers_results.json\")\n",
        "    print(\"âœ… Sentence transformers approach completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“Š COMPARISON AND ANALYSIS\n",
            "==================================================\n",
            "âœ… Custom Implementation Results:\n",
            "   â±ï¸  Time: 12.98s\n",
            "   ğŸ” Duplicates: 5\n",
            "   ğŸ“Š By type: {'exact_url': 4, 'similar_title': 1}\n",
            "\n",
            "âœ… Sentence Transformers Results:\n",
            "   â±ï¸  Time: 4.03s\n",
            "   ğŸ” Duplicates: 9\n",
            "   ğŸ“Š Threshold: 0.8\n",
            "\n",
            "ğŸ’¾ Comparison saved to ../data/deduplication/comparison_summary.json\n",
            "âœ… Analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# COMPARISON AND ANALYSIS\n",
        "print(\"\\nğŸ“Š COMPARISON AND ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load results from both approaches\n",
        "try:\n",
        "    with open('../data/deduplication/custom_implementation_results.json', 'r') as f:\n",
        "        custom_results = json.load(f)\n",
        "    \n",
        "    print(\"âœ… Custom Implementation Results:\")\n",
        "    print(f\"   â±ï¸  Time: {custom_results['processing_time_seconds']:.2f}s\")\n",
        "    print(f\"   ğŸ” Duplicates: {custom_results['total_duplicates']}\")\n",
        "    print(f\"   ğŸ“Š By type: {custom_results['duplicates_by_type']}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Custom implementation results not found\")\n",
        "    custom_results = None\n",
        "\n",
        "if SENTENCE_TRANSFORMERS_AVAILABLE:\n",
        "    try:\n",
        "        with open('../data/deduplication/sentence_transformers_results.json', 'r') as f:\n",
        "            st_results = json.load(f)\n",
        "        \n",
        "        print(\"\\nâœ… Sentence Transformers Results:\")\n",
        "        print(f\"   â±ï¸  Time: {st_results['processing_time_seconds']:.2f}s\")\n",
        "        print(f\"   ğŸ” Duplicates: {st_results['total_duplicates']}\")\n",
        "        print(f\"   ğŸ“Š Threshold: {st_results['similarity_threshold']}\")\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"âŒ Sentence transformers results not found\")\n",
        "        st_results = None\n",
        "else:\n",
        "    st_results = None\n",
        "\n",
        "# Create comparison summary\n",
        "comparison = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'total_articles_processed': len(df),\n",
        "    'approaches': {}\n",
        "}\n",
        "\n",
        "if custom_results:\n",
        "    comparison['approaches']['custom_implementation'] = {\n",
        "        'processing_time_seconds': custom_results['processing_time_seconds'],\n",
        "        'total_duplicates': custom_results['total_duplicates'],\n",
        "        'duplicates_by_type': custom_results['duplicates_by_type'],\n",
        "        'duplicates_per_second': custom_results['total_duplicates'] / custom_results['processing_time_seconds']\n",
        "    }\n",
        "\n",
        "if st_results:\n",
        "    comparison['approaches']['sentence_transformers'] = {\n",
        "        'processing_time_seconds': st_results['processing_time_seconds'],\n",
        "        'total_duplicates': st_results['total_duplicates'],\n",
        "        'similarity_threshold': st_results['similarity_threshold'],\n",
        "        'duplicates_per_second': st_results['total_duplicates'] / st_results['processing_time_seconds']\n",
        "    }\n",
        "\n",
        "# Save comparison\n",
        "with open('../data/deduplication/comparison_summary.json', 'w') as f:\n",
        "    json.dump(comparison, f, indent=2)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Comparison saved to ../data/deduplication/comparison_summary.json\")\n",
        "print(\"âœ… Analysis completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
