{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Content Filtering and Spam Detection\n",
        "\n",
        "This notebook implements various content filtering and processing techniques for our RSS feed data.\n",
        "\n",
        "## Features to Test\n",
        "\n",
        "1. **Spam Detection** - Using `mrm8488/bert-tiny-finetuned-sms-spam-detection` model\n",
        "2. **Content Quality Filtering** - Length, readability, language detection\n",
        "3. **Topic Classification** - Categorize articles by topic\n",
        "4. **Relevance Scoring** - Score articles based on tech relevance\n",
        "5. **Content Cleaning** - Remove unwanted content, normalize text\n",
        "\n",
        "## Goals\n",
        "\n",
        "- Filter out low-quality and spam content\n",
        "- Improve content quality for newsletter generation\n",
        "- Save filtered results in JSON format for analysis\n",
        "- Test different filtering thresholds and approaches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully\n",
            "üì¶ Transformers available: True\n",
            "üì¶ NLTK available: True\n",
            "üì¶ Language detection available: True\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re\n",
        "import os\n",
        "from collections import defaultdict, Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# For transformers and NLP\n",
        "try:\n",
        "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "    import torch\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è transformers not available. Install with: pip install transformers torch\")\n",
        "\n",
        "# For text processing\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    NLTK_AVAILABLE = True\n",
        "except ImportError:\n",
        "    NLTK_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è nltk not available. Install with: pip install nltk\")\n",
        "\n",
        "# For language detection\n",
        "try:\n",
        "    from langdetect import detect, LangDetectException\n",
        "    LANGDETECT_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LANGDETECT_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è langdetect not available. Install with: pip install langdetect\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")\n",
        "print(f\"üì¶ Transformers available: {TRANSFORMERS_AVAILABLE}\")\n",
        "print(f\"üì¶ NLTK available: {NLTK_AVAILABLE}\")\n",
        "print(f\"üì¶ Language detection available: {LANGDETECT_AVAILABLE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Loading RSS data...\n",
            "‚úÖ Loaded 255 articles\n",
            "üìä DataFrame shape: (255, 12)\n",
            "üìù Columns: ['title', 'url', 'summary', 'content', 'published', 'author', 'feed_name', 'feed_category', 'feed_url', 'tags', 'guid', 'raw_entry']\n",
            "\n",
            "üìä Data distribution:\n",
            "   üì∞ Total articles: 255\n",
            "   üè∑Ô∏è Categories: 6\n",
            "   üì° Feeds: 23\n",
            "\n",
            "üìã Sample article:\n",
            "   title: Find out what‚Äôs new in the Gemini app in September‚Äôs Gemini Drop....\n",
            "   feed_name: Google The Keyword...\n",
            "   feed_category: company_blog...\n",
            "   published: 2025-09-19T16:00:00...\n"
          ]
        }
      ],
      "source": [
        "# Load RSS data (full dataset, no deduplication)\n",
        "print(\"üìÇ Loading RSS data...\")\n",
        "with open('../data/rss/rss_data.json', 'r') as f:\n",
        "    rss_data = json.load(f)\n",
        "\n",
        "articles = rss_data['articles']\n",
        "print(f\"‚úÖ Loaded {len(articles)} articles\")\n",
        "\n",
        "# Convert to DataFrame for easier processing\n",
        "df = pd.DataFrame(articles)\n",
        "print(f\"üìä DataFrame shape: {df.shape}\")\n",
        "print(f\"üìù Columns: {list(df.columns)}\")\n",
        "\n",
        "# Show data distribution\n",
        "print(f\"\\nüìä Data distribution:\")\n",
        "print(f\"   üì∞ Total articles: {len(df)}\")\n",
        "print(f\"   üè∑Ô∏è Categories: {len(df['feed_category'].unique())}\")\n",
        "print(f\"   üì° Feeds: {len(df['feed_name'].unique())}\")\n",
        "\n",
        "# Show sample data\n",
        "print(f\"\\nüìã Sample article:\")\n",
        "sample = df.iloc[0]\n",
        "for col in ['title', 'feed_name', 'feed_category', 'published']:\n",
        "    if col in sample:\n",
        "        print(f\"   {col}: {sample[col][:80]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# Helper functions for text processing\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string and strip\n",
        "    text = str(text).strip()\n",
        "    \n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "def get_text_length_stats(text):\n",
        "    \"\"\"Get text length statistics\"\"\"\n",
        "    if not text:\n",
        "        return {'char_count': 0, 'word_count': 0, 'sentence_count': 0}\n",
        "    \n",
        "    char_count = len(text)\n",
        "    word_count = len(text.split())\n",
        "    sentence_count = len(re.split(r'[.!?]+', text))\n",
        "    \n",
        "    return {\n",
        "        'char_count': char_count,\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count\n",
        "    }\n",
        "\n",
        "def detect_language(text):\n",
        "    \"\"\"Detect language of text\"\"\"\n",
        "    if not LANGDETECT_AVAILABLE or not text:\n",
        "        return 'unknown'\n",
        "    \n",
        "    try:\n",
        "        # Use first 500 chars for faster detection\n",
        "        sample_text = text[:500]\n",
        "        return detect(sample_text)\n",
        "    except LangDetectException:\n",
        "        return 'unknown'\n",
        "\n",
        "def calculate_readability_score(text):\n",
        "    \"\"\"Simple readability score based on word/sentence ratio\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    \n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    words = text.split()\n",
        "    \n",
        "    if len(sentences) == 0 or len(words) == 0:\n",
        "        return 0\n",
        "    \n",
        "    avg_words_per_sentence = len(words) / len(sentences)\n",
        "    \n",
        "    # Simple scoring: lower is more readable\n",
        "    if avg_words_per_sentence <= 10:\n",
        "        return 'high'\n",
        "    elif avg_words_per_sentence <= 20:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'low'\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç APPROACH 1: Spam Detection using BERT\n",
            "==================================================\n",
            "üì• Loading BERT spam detection model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "061b3d4922d34bcbba4595e7146343ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31c90086822e4d4496fd04d3017a0d41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/17.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3c9f12feed04081a051c5cc9476e89d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/324 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ebcc6fff8944a18a5af168f402c5202",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92afd449bfee4b1f82424be22a06a160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully\n",
            "üîÑ Preparing text for spam detection...\n",
            "üß† Running spam detection...\n",
            "‚è±Ô∏è  Processing time: 21.19 seconds\n",
            "üîç Spam detection results:\n",
            "   LABEL_0: 217\n",
            "   LABEL_1: 38\n",
            "üíæ Results saved to ../data/filtering/spam_detection_results.json\n",
            "‚úÖ Spam detection completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 1: Spam Detection using BERT\n",
        "print(\"üîç APPROACH 1: Spam Detection using BERT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    print(\"‚ùå Transformers not available. Skipping spam detection.\")\n",
        "    print(\"   Install with: pip install transformers torch\")\n",
        "    spam_results = []\n",
        "else:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load the spam detection model\n",
        "    print(\"üì• Loading BERT spam detection model...\")\n",
        "    try:\n",
        "        spam_classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"mrm8488/bert-tiny-finetuned-sms-spam-detection\",\n",
        "            tokenizer=\"mrm8488/bert-tiny-finetuned-sms-spam-detection\"\n",
        "        )\n",
        "        print(\"‚úÖ Model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        spam_classifier = None\n",
        "    \n",
        "    if spam_classifier:\n",
        "        # Prepare text for spam detection (combine title + content)\n",
        "        print(\"üîÑ Preparing text for spam detection...\")\n",
        "        texts_for_spam = []\n",
        "        for _, row in df.iterrows():\n",
        "            title = clean_text(str(row['title'])) if pd.notna(row['title']) else \"\"\n",
        "            content = clean_text(str(row['content'])) if pd.notna(row['content']) else \"\"\n",
        "            # Combine title and content\n",
        "            combined_text = f\"{title} {content}\".strip()\n",
        "            # Limit length for BERT (512 tokens max)\n",
        "            if len(combined_text) > 500:\n",
        "                combined_text = combined_text[:500] + \"...\"\n",
        "            texts_for_spam.append(combined_text)\n",
        "        \n",
        "        # Run spam detection\n",
        "        print(\"üß† Running spam detection...\")\n",
        "        spam_results = []\n",
        "        \n",
        "        for i, text in enumerate(texts_for_spam):\n",
        "            if text.strip():  # Only process non-empty text\n",
        "                try:\n",
        "                    result = spam_classifier(text)\n",
        "                    spam_results.append({\n",
        "                        'index': i,\n",
        "                        'text': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "                        'prediction': result[0]['label'],\n",
        "                        'confidence': result[0]['score']\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    spam_results.append({\n",
        "                        'index': i,\n",
        "                        'text': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "                        'prediction': 'error',\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e)\n",
        "                    })\n",
        "            else:\n",
        "                spam_results.append({\n",
        "                    'index': i,\n",
        "                    'text': '',\n",
        "                    'prediction': 'empty',\n",
        "                    'confidence': 0.0\n",
        "                })\n",
        "        \n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        \n",
        "        # Analyze results\n",
        "        predictions = [r['prediction'] for r in spam_results]\n",
        "        prediction_counts = Counter(predictions)\n",
        "        \n",
        "        print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds\")\n",
        "        print(f\"üîç Spam detection results:\")\n",
        "        for pred, count in prediction_counts.items():\n",
        "            print(f\"   {pred}: {count}\")\n",
        "        \n",
        "        # Show some examples\n",
        "        spam_examples = [r for r in spam_results if r['prediction'] == 'spam']\n",
        "        if spam_examples:\n",
        "            print(f\"\\nüìã Sample spam detected ({len(spam_examples)} total):\")\n",
        "            for i, example in enumerate(spam_examples[:3]):\n",
        "                print(f\"   {i+1}. Confidence: {example['confidence']:.3f}\")\n",
        "                print(f\"      Text: {example['text']}\")\n",
        "                print()\n",
        "        \n",
        "        # Save results\n",
        "        spam_analysis = {\n",
        "            'approach': 'bert_spam_detection',\n",
        "            'model': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'processing_time_seconds': processing_time,\n",
        "            'total_articles': len(df),\n",
        "            'prediction_counts': dict(prediction_counts),\n",
        "            'spam_results': spam_results\n",
        "        }\n",
        "        \n",
        "        os.makedirs('../data/filtering', exist_ok=True)\n",
        "        with open('../data/filtering/spam_detection_results.json', 'w') as f:\n",
        "            json.dump(spam_analysis, f, indent=2)\n",
        "        \n",
        "        print(f\"üíæ Results saved to ../data/filtering/spam_detection_results.json\")\n",
        "        print(\"‚úÖ Spam detection completed!\")\n",
        "    else:\n",
        "        spam_results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç APPROACH 2: Content Quality Filtering\n",
            "==================================================\n",
            "‚è±Ô∏è  Processing time: 11.08 seconds\n",
            "üîç Content quality results:\n",
            "   low: 196\n",
            "   medium: 46\n",
            "   high: 13\n",
            "\n",
            "üìä Quality issues found:\n",
            "   too_short_content: 202\n",
            "   empty_content: 194\n",
            "   excessive_caps: 28\n",
            "   poor_readability: 27\n",
            "   non_english: 6\n",
            "   poor_title: 1\n",
            "\n",
            "üìã Sample low quality articles (196 total):\n",
            "   1. Score: -5, Issues: ['too_short_content', 'empty_content']\n",
            "      Title: Find out what‚Äôs new in the Gemini app in September‚Äôs Gemini Drop.\n",
            "\n",
            "   2. Score: -5, Issues: ['too_short_content', 'empty_content']\n",
            "      Title: DOJ's remedies go significantly beyond the Court's ruling and would harm publishers and advertisers.\n",
            "\n",
            "   3. Score: -3, Issues: ['too_short_content', 'poor_readability']\n",
            "      Title: Go behind the browser with Chrome‚Äôs new AI features\n",
            "\n",
            "üíæ Results saved to ../data/filtering/content_quality_results.json\n",
            "‚úÖ Content quality filtering completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 2: Content Quality Filtering\n",
        "print(\"\\nüîç APPROACH 2: Content Quality Filtering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Analyze content quality for each article\n",
        "quality_results = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    title = clean_text(str(row['title'])) if pd.notna(row['title']) else \"\"\n",
        "    content = clean_text(str(row['content'])) if pd.notna(row['content']) else \"\"\n",
        "    summary = clean_text(str(row['summary'])) if pd.notna(row['summary']) else \"\"\n",
        "    \n",
        "    # Get text statistics\n",
        "    title_stats = get_text_length_stats(title)\n",
        "    content_stats = get_text_length_stats(content)\n",
        "    summary_stats = get_text_length_stats(summary)\n",
        "    \n",
        "    # Detect language\n",
        "    combined_text = f\"{title} {content}\".strip()\n",
        "    language = detect_language(combined_text)\n",
        "    \n",
        "    # Calculate readability\n",
        "    readability = calculate_readability_score(combined_text)\n",
        "    \n",
        "    # Quality scoring\n",
        "    quality_score = 0\n",
        "    quality_issues = []\n",
        "    \n",
        "    # Check minimum content length\n",
        "    if content_stats['word_count'] < 50:\n",
        "        quality_score -= 2\n",
        "        quality_issues.append('too_short_content')\n",
        "    \n",
        "    # Check title quality\n",
        "    if title_stats['word_count'] < 3:\n",
        "        quality_score -= 2\n",
        "        quality_issues.append('poor_title')\n",
        "    \n",
        "    # Check language\n",
        "    if language != 'en':\n",
        "        quality_score -= 1\n",
        "        quality_issues.append('non_english')\n",
        "    \n",
        "    # Check readability\n",
        "    if readability == 'low':\n",
        "        quality_score -= 1\n",
        "        quality_issues.append('poor_readability')\n",
        "    \n",
        "    # Check for empty content\n",
        "    if not content.strip():\n",
        "        quality_score -= 3\n",
        "        quality_issues.append('empty_content')\n",
        "    \n",
        "    # Check for suspicious patterns\n",
        "    if re.search(r'(click here|buy now|limited time|act now)', combined_text.lower()):\n",
        "        quality_score -= 2\n",
        "        quality_issues.append('promotional_language')\n",
        "    \n",
        "    # Check for excessive capitalization\n",
        "    if len(re.findall(r'[A-Z]{3,}', combined_text)) > 3:\n",
        "        quality_score -= 1\n",
        "        quality_issues.append('excessive_caps')\n",
        "    \n",
        "    # Assign quality level\n",
        "    if quality_score >= 0:\n",
        "        quality_level = 'high'\n",
        "    elif quality_score >= -2:\n",
        "        quality_level = 'medium'\n",
        "    else:\n",
        "        quality_level = 'low'\n",
        "    \n",
        "    quality_results.append({\n",
        "        'index': i,\n",
        "        'title': title[:100] + \"...\" if len(title) > 100 else title,\n",
        "        'quality_score': quality_score,\n",
        "        'quality_level': quality_level,\n",
        "        'quality_issues': quality_issues,\n",
        "        'language': language,\n",
        "        'readability': readability,\n",
        "        'title_stats': title_stats,\n",
        "        'content_stats': content_stats,\n",
        "        'summary_stats': summary_stats,\n",
        "        'feed_name': row['feed_name'],\n",
        "        'feed_category': row['feed_category']\n",
        "    })\n",
        "\n",
        "end_time = time.time()\n",
        "processing_time = end_time - start_time\n",
        "\n",
        "# Analyze results\n",
        "quality_levels = [r['quality_level'] for r in quality_results]\n",
        "quality_counts = Counter(quality_levels)\n",
        "\n",
        "print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds\")\n",
        "print(f\"üîç Content quality results:\")\n",
        "for level, count in quality_counts.items():\n",
        "    print(f\"   {level}: {count}\")\n",
        "\n",
        "# Show quality issues\n",
        "all_issues = []\n",
        "for r in quality_results:\n",
        "    all_issues.extend(r['quality_issues'])\n",
        "issue_counts = Counter(all_issues)\n",
        "\n",
        "print(f\"\\nüìä Quality issues found:\")\n",
        "for issue, count in issue_counts.most_common():\n",
        "    print(f\"   {issue}: {count}\")\n",
        "\n",
        "# Show examples of low quality content\n",
        "low_quality = [r for r in quality_results if r['quality_level'] == 'low']\n",
        "if low_quality:\n",
        "    print(f\"\\nüìã Sample low quality articles ({len(low_quality)} total):\")\n",
        "    for i, example in enumerate(low_quality[:3]):\n",
        "        print(f\"   {i+1}. Score: {example['quality_score']}, Issues: {example['quality_issues']}\")\n",
        "        print(f\"      Title: {example['title']}\")\n",
        "        print()\n",
        "\n",
        "# Save results\n",
        "quality_analysis = {\n",
        "    'approach': 'content_quality_filtering',\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'processing_time_seconds': processing_time,\n",
        "    'total_articles': len(df),\n",
        "    'quality_distribution': dict(quality_counts),\n",
        "    'issue_distribution': dict(issue_counts),\n",
        "    'quality_results': quality_results\n",
        "}\n",
        "\n",
        "with open('../data/filtering/content_quality_results.json', 'w') as f:\n",
        "    json.dump(quality_analysis, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Results saved to ../data/filtering/content_quality_results.json\")\n",
        "print(\"‚úÖ Content quality filtering completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä COMPREHENSIVE FILTERING AND ANALYSIS\n",
            "==================================================\n",
            "üìä Filtering Results:\n",
            "   üì∞ Total articles: 255\n",
            "   üö´ Spam filtered: 0\n",
            "   üìâ Low quality filtered: 196\n",
            "   ‚úÖ Passed filters: 59\n",
            "\n",
            "üìã Filter reasons:\n",
            "   low_quality: 196\n",
            "\n",
            "üìà Filtering efficiency: 76.9% of articles filtered\n",
            "\n",
            "üíæ Comprehensive results saved to ../data/filtering/comprehensive_filtering_results.json\n",
            "‚úÖ Comprehensive filtering analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# COMPREHENSIVE FILTERING AND ANALYSIS\n",
        "print(\"\\nüìä COMPREHENSIVE FILTERING AND ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Combine spam detection and quality filtering results\n",
        "filtered_articles = []\n",
        "filter_stats = {\n",
        "    'total_articles': len(df),\n",
        "    'spam_filtered': 0,\n",
        "    'low_quality_filtered': 0,\n",
        "    'passed_filters': 0,\n",
        "    'filter_reasons': defaultdict(int)\n",
        "}\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    article_data = {\n",
        "        'index': i,\n",
        "        'title': row['title'],\n",
        "        'url': row['url'],\n",
        "        'feed_name': row['feed_name'],\n",
        "        'feed_category': row['feed_category'],\n",
        "        'published': row['published'],\n",
        "        'content': row['content'],\n",
        "        'summary': row['summary']\n",
        "    }\n",
        "    \n",
        "    # Check spam detection results\n",
        "    spam_result = None\n",
        "    if spam_results and i < len(spam_results):\n",
        "        spam_result = spam_results[i]\n",
        "        if spam_result['prediction'] == 'spam' and spam_result['confidence'] > 0.7:\n",
        "            filter_stats['spam_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['spam'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Check quality filtering results\n",
        "    quality_result = None\n",
        "    if i < len(quality_results):\n",
        "        quality_result = quality_results[i]\n",
        "        if quality_result['quality_level'] == 'low':\n",
        "            filter_stats['low_quality_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['low_quality'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Article passed all filters\n",
        "    article_data['spam_analysis'] = spam_result\n",
        "    article_data['quality_analysis'] = quality_result\n",
        "    filtered_articles.append(article_data)\n",
        "    filter_stats['passed_filters'] += 1\n",
        "\n",
        "print(f\"üìä Filtering Results:\")\n",
        "print(f\"   üì∞ Total articles: {filter_stats['total_articles']}\")\n",
        "print(f\"   üö´ Spam filtered: {filter_stats['spam_filtered']}\")\n",
        "print(f\"   üìâ Low quality filtered: {filter_stats['low_quality_filtered']}\")\n",
        "print(f\"   ‚úÖ Passed filters: {filter_stats['passed_filters']}\")\n",
        "\n",
        "print(f\"\\nüìã Filter reasons:\")\n",
        "for reason, count in filter_stats['filter_reasons'].items():\n",
        "    print(f\"   {reason}: {count}\")\n",
        "\n",
        "# Calculate filtering efficiency\n",
        "filtering_rate = (filter_stats['spam_filtered'] + filter_stats['low_quality_filtered']) / filter_stats['total_articles'] * 100\n",
        "print(f\"\\nüìà Filtering efficiency: {filtering_rate:.1f}% of articles filtered\")\n",
        "\n",
        "# Save comprehensive results\n",
        "comprehensive_results = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'filter_stats': dict(filter_stats),\n",
        "    'filtering_rate_percent': filtering_rate,\n",
        "    'filtered_articles': filtered_articles,\n",
        "    'spam_detection_summary': {\n",
        "        'model_used': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
        "        'total_analyzed': len(spam_results) if spam_results else 0,\n",
        "        'spam_detected': filter_stats['spam_filtered']\n",
        "    },\n",
        "    'quality_filtering_summary': {\n",
        "        'total_analyzed': len(quality_results),\n",
        "        'low_quality_filtered': filter_stats['low_quality_filtered']\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('../data/filtering/comprehensive_filtering_results.json', 'w') as f:\n",
        "    json.dump(comprehensive_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Comprehensive results saved to ../data/filtering/comprehensive_filtering_results.json\")\n",
        "print(\"‚úÖ Comprehensive filtering analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç APPROACH 3: Ad Detection using BERT\n",
            "==================================================\n",
            "üì• Loading BERT ad detection model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf5b63620d9040949bac6d0e2eff4730",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d877cde42a2e40ff9e557c25c9200f63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f46e51ca1a934d16983c9f741d460b18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a27f142ee43045abb5b087c941db6395",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f18d51a67b74da8858683a29a46b4f3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b00fe8ca57c04ab6bf01719f1898b32b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6cdead87f86a4d6a9df7605a6dd53400",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ BERT ad detection model loaded successfully\n",
            "üîÑ Preparing text for ad detection...\n",
            "üß† Running ad detection...\n",
            "‚è±Ô∏è  Processing time: 22.45 seconds\n",
            "üîç Ad detection results:\n",
            "   LABEL_1: 170\n",
            "   LABEL_0: 85\n",
            "üíæ Results saved to ../data/filtering/ad_detection_results.json\n",
            "‚úÖ Ad detection completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 3: Ad Detection using BERT\n",
        "print(\"\\nüîç APPROACH 3: Ad Detection using BERT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    print(\"‚ùå Transformers not available. Skipping ad detection.\")\n",
        "    print(\"   Install with: pip install transformers torch\")\n",
        "    ad_results = []\n",
        "else:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load the BERT ad detection model\n",
        "    print(\"üì• Loading BERT ad detection model...\")\n",
        "    try:\n",
        "        ad_classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"bondarchukb/bert-ads-classification\",\n",
        "            tokenizer=\"bondarchukb/bert-ads-classification\"\n",
        "        )\n",
        "        print(\"‚úÖ BERT ad detection model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        raise Exception(f\"Failed to load ad detection model: {e}\")\n",
        "    \n",
        "    if ad_classifier:\n",
        "        # Prepare text for ad detection (combine title + content)\n",
        "        print(\"üîÑ Preparing text for ad detection...\")\n",
        "        texts_for_ads = []\n",
        "        for _, row in df.iterrows():\n",
        "            title = clean_text(str(row['title'])) if pd.notna(row['title']) else \"\"\n",
        "            content = clean_text(str(row['content'])) if pd.notna(row['content']) else \"\"\n",
        "            # Combine title and content\n",
        "            combined_text = f\"{title} {content}\".strip()\n",
        "            # Limit length for RoBERTa (512 tokens max)\n",
        "            if len(combined_text) > 500:\n",
        "                combined_text = combined_text[:500] + \"...\"\n",
        "            texts_for_ads.append(combined_text)\n",
        "        \n",
        "        # Run ad detection\n",
        "        print(\"üß† Running ad detection...\")\n",
        "        ad_results = []\n",
        "        \n",
        "        for i, text in enumerate(texts_for_ads):\n",
        "            if text.strip():  # Only process non-empty text\n",
        "                try:\n",
        "                    result = ad_classifier(text)\n",
        "                    ad_results.append({\n",
        "                        'index': i,\n",
        "                        'text': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "                        'prediction': result[0]['label'],\n",
        "                        'confidence': result[0]['score']\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    ad_results.append({\n",
        "                        'index': i,\n",
        "                        'text': text[:200] + \"...\" if len(text) > 200 else text,\n",
        "                        'prediction': 'error',\n",
        "                        'confidence': 0.0,\n",
        "                        'error': str(e)\n",
        "                    })\n",
        "            else:\n",
        "                ad_results.append({\n",
        "                    'index': i,\n",
        "                    'text': '',\n",
        "                    'prediction': 'empty',\n",
        "                    'confidence': 0.0\n",
        "                })\n",
        "        \n",
        "        end_time = time.time()\n",
        "        processing_time = end_time - start_time\n",
        "        \n",
        "        # Analyze results\n",
        "        predictions = [r['prediction'] for r in ad_results]\n",
        "        prediction_counts = Counter(predictions)\n",
        "        \n",
        "        print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds\")\n",
        "        print(f\"üîç Ad detection results:\")\n",
        "        for pred, count in prediction_counts.items():\n",
        "            print(f\"   {pred}: {count}\")\n",
        "        \n",
        "        # Show some examples\n",
        "        ad_examples = [r for r in ad_results if r['prediction'] == 'ad']\n",
        "        if ad_examples:\n",
        "            print(f\"\\nüìã Sample ads detected ({len(ad_examples)} total):\")\n",
        "            for i, example in enumerate(ad_examples[:3]):\n",
        "                print(f\"   {i+1}. Confidence: {example['confidence']:.3f}\")\n",
        "                print(f\"      Text: {example['text']}\")\n",
        "                print()\n",
        "        \n",
        "        # Save results\n",
        "        ad_analysis = {\n",
        "            'approach': 'bert_ad_detection',\n",
        "            'model': 'bondarchukb/bert-ads-classification',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'processing_time_seconds': processing_time,\n",
        "            'total_articles': len(df),\n",
        "            'prediction_counts': dict(prediction_counts),\n",
        "            'ad_results': ad_results\n",
        "        }\n",
        "        \n",
        "        with open('../data/filtering/ad_detection_results.json', 'w') as f:\n",
        "            json.dump(ad_analysis, f, indent=2)\n",
        "        \n",
        "        print(f\"üíæ Results saved to ../data/filtering/ad_detection_results.json\")\n",
        "        print(\"‚úÖ Ad detection completed!\")\n",
        "    else:\n",
        "        ad_results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç APPROACH 4: News Classification using RoBERTa\n",
            "==================================================\n",
            "üì• Loading RoBERTa news classification model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2aa8cbdafa3546dd89e59ff6b27f4c4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5b1aecc39d04a13afd97c0e43d7acba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b53da26a269c4f0e8c4cd064e107259c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "454c1f5c95cc4152af4562b345880a6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25edf51e8d8548798e91b64de353008e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5224c6114deb41eabb063d8126eff5df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7bf56061bb946c6b77d1828986f33b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52beffc5707c45ff850700a93a46ce57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/957 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RoBERTa news classification model loaded successfully\n",
            "üîÑ Preparing text for news classification...\n",
            "üß† Running news classification...\n",
            "‚è±Ô∏è  Processing time: 32.03 seconds\n",
            "üîç News classification results:\n",
            "   LABEL_6: 169 (66.3%)\n",
            "   LABEL_2: 63 (24.7%)\n",
            "   LABEL_1: 13 (5.1%)\n",
            "   LABEL_3: 10 (3.9%)\n",
            "\n",
            "üìä News vs Non-News Summary:\n",
            "   üì∞ News articles: 0 (0.0%)\n",
            "   üìÑ Non-news articles: 255 (100.0%)\n",
            "\n",
            "üè∑Ô∏è Top News Categories:\n",
            "üíæ Results saved to ../data/filtering/news_classification_results.json\n",
            "‚úÖ News classification completed!\n"
          ]
        }
      ],
      "source": [
        "# APPROACH 4: News Classification using RoBERTa\n",
        "print(\"\\nüîç APPROACH 4: News Classification using RoBERTa\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    print(\"‚ùå Transformers not available. Skipping news classification.\")\n",
        "    print(\"   Install with: pip install transformers torch\")\n",
        "    news_results = []\n",
        "else:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load the RoBERTa news classification model\n",
        "    print(\"üì• Loading RoBERTa news classification model...\")\n",
        "    try:\n",
        "        news_classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"resul-ai/roberta-news-classifier\",\n",
        "            tokenizer=\"resul-ai/roberta-news-classifier\"\n",
        "        )\n",
        "        print(\"‚úÖ RoBERTa news classification model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        raise Exception(f\"Failed to load news classification model: {e}\")\n",
        "    \n",
        "    if news_classifier:\n",
        "        # Prepare text for news classification\n",
        "        print(\"üîÑ Preparing text for news classification...\")\n",
        "        texts_to_classify = []\n",
        "        for i, row in df.iterrows():\n",
        "            # Combine title and content for better classification\n",
        "            text = f\"{row['title']} {row.get('summary', '')} {row.get('content', '')}\"\n",
        "            cleaned_text = clean_text(text)[:500]  # Limit to 500 chars for efficiency\n",
        "            texts_to_classify.append(cleaned_text)\n",
        "        \n",
        "        # Run news classification\n",
        "        print(\"üß† Running news classification...\")\n",
        "        news_results = []\n",
        "        prediction_counts = defaultdict(int)\n",
        "        \n",
        "        for i, text in enumerate(texts_to_classify):\n",
        "            try:\n",
        "                # Get classification result\n",
        "                result = news_classifier(text)\n",
        "                \n",
        "                # Extract prediction\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    prediction = result[0]\n",
        "                    label = prediction['label']\n",
        "                    score = prediction['score']\n",
        "                else:\n",
        "                    label = \"unknown\"\n",
        "                    score = 0.0\n",
        "                \n",
        "                # Store result\n",
        "                news_results.append({\n",
        "                    'index': i,\n",
        "                    'title': df.iloc[i]['title'],\n",
        "                    'feed_name': df.iloc[i]['feed_name'],\n",
        "                    'feed_category': df.iloc[i]['feed_category'],\n",
        "                    'text_analyzed': text[:100] + \"...\" if len(text) > 100 else text,\n",
        "                    'predicted_category': label,\n",
        "                    'confidence_score': score,\n",
        "                    'is_news': label in ['teknoloji', 'siyaset', 'dunya', 'ekonomi', 'saglik', 'spor', 'kultur']  # Turkish labels from the model\n",
        "                })\n",
        "                \n",
        "                prediction_counts[label] += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ö†Ô∏è Error classifying article {i}: {e}\")\n",
        "                news_results.append({\n",
        "                    'index': i,\n",
        "                    'title': df.iloc[i]['title'],\n",
        "                    'feed_name': df.iloc[i]['feed_name'],\n",
        "                    'feed_category': df.iloc[i]['feed_category'],\n",
        "                    'text_analyzed': text[:100] + \"...\" if len(text) > 100 else text,\n",
        "                    'predicted_category': 'error',\n",
        "                    'confidence_score': 0.0,\n",
        "                    'is_news': False\n",
        "                })\n",
        "                prediction_counts['error'] += 1\n",
        "        \n",
        "        processing_time = time.time() - start_time\n",
        "        \n",
        "        # Display results\n",
        "        print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds\")\n",
        "        print(\"üîç News classification results:\")\n",
        "        for label, count in sorted(prediction_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"   {label}: {count} ({percentage:.1f}%)\")\n",
        "        \n",
        "        # Calculate news vs non-news statistics\n",
        "        news_count = sum(1 for r in news_results if r['is_news'])\n",
        "        non_news_count = len(news_results) - news_count\n",
        "        \n",
        "        print(f\"\\nüìä News vs Non-News Summary:\")\n",
        "        print(f\"   üì∞ News articles: {news_count} ({(news_count/len(df)*100):.1f}%)\")\n",
        "        print(f\"   üìÑ Non-news articles: {non_news_count} ({(non_news_count/len(df)*100):.1f}%)\")\n",
        "        \n",
        "        # Show top categories\n",
        "        print(f\"\\nüè∑Ô∏è Top News Categories:\")\n",
        "        tech_articles = [r for r in news_results if r['predicted_category'] == 'teknoloji']\n",
        "        if tech_articles:\n",
        "            print(f\"   üîß Technology: {len(tech_articles)} articles\")\n",
        "        \n",
        "        politics_articles = [r for r in news_results if r['predicted_category'] == 'siyaset']\n",
        "        if politics_articles:\n",
        "            print(f\"   üèõÔ∏è Politics: {len(politics_articles)} articles\")\n",
        "        \n",
        "        world_articles = [r for r in news_results if r['predicted_category'] == 'dunya']\n",
        "        if world_articles:\n",
        "            print(f\"   üåç World: {len(world_articles)} articles\")\n",
        "        \n",
        "        # Save results\n",
        "        news_analysis = {\n",
        "            'approach': 'roberta_news_classification',\n",
        "            'model': 'resul-ai/roberta-news-classifier',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'processing_time_seconds': processing_time,\n",
        "            'total_articles': len(df),\n",
        "            'prediction_counts': dict(prediction_counts),\n",
        "            'news_statistics': {\n",
        "                'total_news': news_count,\n",
        "                'total_non_news': non_news_count,\n",
        "                'news_percentage': (news_count/len(df)*100),\n",
        "                'non_news_percentage': (non_news_count/len(df)*100)\n",
        "            },\n",
        "            'news_results': news_results\n",
        "        }\n",
        "        \n",
        "        # Create news classification directory\n",
        "        os.makedirs('../data/filtering', exist_ok=True)\n",
        "        \n",
        "        with open('../data/filtering/news_classification_results.json', 'w') as f:\n",
        "            json.dump(news_analysis, f, indent=2)\n",
        "        \n",
        "        print(f\"üíæ Results saved to ../data/filtering/news_classification_results.json\")\n",
        "        print(\"‚úÖ News classification completed!\")\n",
        "    else:\n",
        "        news_results = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UPDATED COMPREHENSIVE FILTERING AND ANALYSIS\n",
        "print(\"\\nüìä UPDATED COMPREHENSIVE FILTERING AND ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Combine spam detection, quality filtering, ad detection, and news classification results\n",
        "filtered_articles = []\n",
        "filter_stats = {\n",
        "    'total_articles': len(df),\n",
        "    'spam_filtered': 0,\n",
        "    'low_quality_filtered': 0,\n",
        "    'ad_filtered': 0,\n",
        "    'non_news_filtered': 0,\n",
        "    'passed_filters': 0,\n",
        "    'filter_reasons': defaultdict(int)\n",
        "}\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    article_data = {\n",
        "        'index': i,\n",
        "        'title': row['title'],\n",
        "        'url': row['url'],\n",
        "        'feed_name': row['feed_name'],\n",
        "        'feed_category': row['feed_category'],\n",
        "        'published': row['published'],\n",
        "        'content': row['content'],\n",
        "        'summary': row['summary']\n",
        "    }\n",
        "    \n",
        "    # Check spam detection results\n",
        "    spam_result = None\n",
        "    if spam_results and i < len(spam_results):\n",
        "        spam_result = spam_results[i]\n",
        "        if spam_result['prediction'] == 'LABEL_1' and spam_result['confidence'] > 0.7:\n",
        "            filter_stats['spam_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['spam'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Check ad detection results\n",
        "    ad_result = None\n",
        "    if ad_results and i < len(ad_results):\n",
        "        ad_result = ad_results[i]\n",
        "        if ad_result['prediction'] == 'ad' and ad_result['confidence'] > 0.7:\n",
        "            filter_stats['ad_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['ad'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Check quality filtering results\n",
        "    quality_result = None\n",
        "    if i < len(quality_results):\n",
        "        quality_result = quality_results[i]\n",
        "        if quality_result['quality_level'] == 'low':\n",
        "            filter_stats['low_quality_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['low_quality'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Check news classification results\n",
        "    news_result = None\n",
        "    if news_results and i < len(news_results):\n",
        "        news_result = news_results[i]\n",
        "        if news_result['is_news'] == False:\n",
        "            filter_stats['non_news_filtered'] += 1\n",
        "            filter_stats['filter_reasons']['non_news'] += 1\n",
        "            continue\n",
        "    \n",
        "    # Article passed all filters\n",
        "    article_data['spam_analysis'] = spam_result\n",
        "    article_data['ad_analysis'] = ad_result\n",
        "    article_data['quality_analysis'] = quality_result\n",
        "    article_data['news_analysis'] = news_result\n",
        "    filtered_articles.append(article_data)\n",
        "    filter_stats['passed_filters'] += 1\n",
        "\n",
        "print(f\"üìä Updated Filtering Results:\")\n",
        "print(f\"   üì∞ Total articles: {filter_stats['total_articles']}\")\n",
        "print(f\"   üö´ Spam filtered: {filter_stats['spam_filtered']}\")\n",
        "print(f\"   üì¢ Ad filtered: {filter_stats['ad_filtered']}\")\n",
        "print(f\"   üìâ Low quality filtered: {filter_stats['low_quality_filtered']}\")\n",
        "print(f\"   üìÑ Non-news filtered: {filter_stats['non_news_filtered']}\")\n",
        "print(f\"   ‚úÖ Passed filters: {filter_stats['passed_filters']}\")\n",
        "\n",
        "print(f\"\\nüìã Filter reasons:\")\n",
        "for reason, count in filter_stats['filter_reasons'].items():\n",
        "    print(f\"   {reason}: {count}\")\n",
        "\n",
        "# Calculate filtering efficiency\n",
        "total_filtered = filter_stats['spam_filtered'] + filter_stats['low_quality_filtered'] + filter_stats['ad_filtered'] + filter_stats['non_news_filtered']\n",
        "filtering_rate = total_filtered / filter_stats['total_articles'] * 100\n",
        "print(f\"\\nüìà Total filtering efficiency: {filtering_rate:.1f}% of articles filtered\")\n",
        "\n",
        "# Save updated comprehensive results\n",
        "updated_comprehensive_results = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'filter_stats': dict(filter_stats),\n",
        "    'filtering_rate_percent': filtering_rate,\n",
        "    'filtered_articles': filtered_articles,\n",
        "    'spam_detection_summary': {\n",
        "        'model_used': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
        "        'total_analyzed': len(spam_results) if spam_results else 0,\n",
        "        'spam_detected': filter_stats['spam_filtered']\n",
        "    },\n",
        "    'ad_detection_summary': {\n",
        "        'model_used': '0x7o/roberta-base-ad-detector',\n",
        "        'total_analyzed': len(ad_results) if ad_results else 0,\n",
        "        'ads_detected': filter_stats['ad_filtered']\n",
        "    },\n",
        "    'quality_filtering_summary': {\n",
        "        'total_analyzed': len(quality_results),\n",
        "        'low_quality_filtered': filter_stats['low_quality_filtered']\n",
        "    },\n",
        "    'news_classification_summary': {\n",
        "        'model_used': 'resul-ai/roberta-news-classifier',\n",
        "        'total_analyzed': len(news_results) if news_results else 0,\n",
        "        'non_news_filtered': filter_stats['non_news_filtered']\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('../data/filtering/updated_comprehensive_filtering_results.json', 'w') as f:\n",
        "    json.dump(updated_comprehensive_results, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Updated comprehensive results saved to ../data/filtering/updated_comprehensive_filtering_results.json\")\n",
        "print(\"‚úÖ Updated comprehensive filtering analysis completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
