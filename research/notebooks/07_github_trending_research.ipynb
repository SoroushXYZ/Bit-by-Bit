{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GitHub Trending Projects Research\n",
        "\n",
        "This notebook explores the OSS Insight API to gather trending GitHub projects from the past 24 hours.\n",
        "\n",
        "## Objectives\n",
        "- Test the OSS Insight API for trending repositories\n",
        "- Gather data from past 24 hours (all languages)\n",
        "- Analyze the data structure and quality\n",
        "- Explore potential integration with our newsletter pipeline\n",
        "\n",
        "## API Endpoint\n",
        "- **Base URL**: `https://api.ossinsight.io/v1/trends/repos/`\n",
        "- **Parameters**: \n",
        "  - `period=past_24_hours`\n",
        "  - `language=All` (default, all languages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Set up display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó API URL: https://api.ossinsight.io/v1/trends/repos/?period=past_24_hours&language=All\n",
            "üìÖ Period: past_24_hours\n",
            "üåê Language: All\n",
            "‚è∞ Request time: 2025-09-28 23:57:48\n"
          ]
        }
      ],
      "source": [
        "# API Configuration\n",
        "API_BASE_URL = \"https://api.ossinsight.io/v1/trends/repos/\"\n",
        "PERIOD = \"past_24_hours\"\n",
        "LANGUAGE = \"All\"  # All languages\n",
        "\n",
        "# Build the request URL\n",
        "url = f\"{API_BASE_URL}?period={PERIOD}&language={LANGUAGE}\"\n",
        "\n",
        "print(f\"üîó API URL: {url}\")\n",
        "print(f\"üìÖ Period: {PERIOD}\")\n",
        "print(f\"üåê Language: {LANGUAGE}\")\n",
        "print(f\"‚è∞ Request time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Making API request...\n",
            "‚úÖ Request successful! (0.44s)\n",
            "üìä Status Code: 200\n",
            "üìè Response Size: 35,342 bytes\n"
          ]
        }
      ],
      "source": [
        "# Make the API request\n",
        "print(\"üöÄ Making API request...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    \n",
        "    request_time = time.time() - start_time\n",
        "    print(f\"‚úÖ Request successful! ({request_time:.2f}s)\")\n",
        "    print(f\"üìä Status Code: {response.status_code}\")\n",
        "    print(f\"üìè Response Size: {len(response.content):,} bytes\")\n",
        "    \n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Request failed: {e}\")\n",
        "    response = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ JSON parsing successful!\n",
            "\n",
            "üìã Response Structure:\n",
            "   Type: sql_endpoint\n",
            "   Data keys: ['columns', 'rows', 'result']\n",
            "   Number of repositories: 100\n",
            "   Number of columns: 11\n",
            "   Columns: ['repo_id', 'repo_name', 'primary_language', 'description', 'stars', 'forks', 'pull_requests', 'pushes', 'total_score', 'contributor_logins', 'collection_names']\n"
          ]
        }
      ],
      "source": [
        "# Parse and examine the response\n",
        "if response:\n",
        "    try:\n",
        "        data = response.json()\n",
        "        print(\"‚úÖ JSON parsing successful!\")\n",
        "        \n",
        "        # Examine the structure\n",
        "        print(f\"\\nüìã Response Structure:\")\n",
        "        print(f\"   Type: {data.get('type', 'Unknown')}\")\n",
        "        \n",
        "        if 'data' in data:\n",
        "            data_section = data['data']\n",
        "            print(f\"   Data keys: {list(data_section.keys())}\")\n",
        "            \n",
        "            # Check if we have rows\n",
        "            if 'rows' in data_section:\n",
        "                rows = data_section['rows']\n",
        "                print(f\"   Number of repositories: {len(rows)}\")\n",
        "                \n",
        "                # Show column information\n",
        "                if 'columns' in data_section:\n",
        "                    columns = data_section['columns']\n",
        "                    print(f\"   Number of columns: {len(columns)}\")\n",
        "                    print(f\"   Columns: {[col['col'] for col in columns]}\")\n",
        "        \n",
        "        # Show result metadata\n",
        "        if 'result' in data:\n",
        "            result = data['result']\n",
        "            print(f\"\\nüìä Query Result:\")\n",
        "            print(f\"   Code: {result.get('code', 'N/A')}\")\n",
        "            print(f\"   Message: {result.get('message', 'N/A')}\")\n",
        "            print(f\"   Latency: {result.get('latency', 'N/A')}\")\n",
        "            print(f\"   Row Count: {result.get('row_count', 'N/A')}\")\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå JSON parsing failed: {e}\")\n",
        "        data = None\n",
        "else:\n",
        "    print(\"‚ùå No response to parse\")\n",
        "    data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä DataFrame created with 100 rows and 11 columns\n",
            "üìã Columns: ['repo_id', 'repo_name', 'primary_language', 'description', 'stars', 'forks', 'pull_requests', 'pushes', 'total_score', 'contributor_logins', 'collection_names']\n",
            "\n",
            "üîç First 5 repositories:\n",
            "      repo_id                           repo_name primary_language  \\\n",
            "0  1054793726  ChromeDevTools/chrome-devtools-mcp       TypeScript   \n",
            "1   838542536               humanlayer/humanlayer       TypeScript   \n",
            "2  1064789156                yasadEv/spyder-osint           Python   \n",
            "3   997220241                  HKUDS/RAG-Anything           Python   \n",
            "4  1042367133                     github/spec-kit           Python   \n",
            "\n",
            "                                                                         description  \\\n",
            "0                                                  Chrome DevTools for coding agents   \n",
            "1  The best way to get AI coding agents to solve hard problems in complex codebases.   \n",
            "2                                                             A powerful osint tool.   \n",
            "3                                           \"RAG-Anything: All-in-One RAG Framework\"   \n",
            "4                     üí´ Toolkit to help you get started with Spec-Driven Development   \n",
            "\n",
            "  stars forks pull_requests pushes total_score  \\\n",
            "0  1082    49             2          4194.7945   \n",
            "1   621    32             3      7   2455.1433   \n",
            "2   376   206                        2017.4226   \n",
            "3   398    45                        1756.9536   \n",
            "4   351    34             2          1576.1235   \n",
            "\n",
            "                                            contributor_logins  \\\n",
            "0      kmk142789,zhouhoulai,ronantakizawa,rechardlc,starkshade   \n",
            "1  K-Mistele,dexhorthy,kmk142789,mdmonir868017957-crypto,0xjgv   \n",
            "2                                                                \n",
            "3                                                                \n",
            "4              crisweber2600,deepak-vij,dlukt,Calel33,djsekops   \n",
            "\n",
            "  collection_names  \n",
            "0                   \n",
            "1                   \n",
            "2                   \n",
            "3                   \n",
            "4                   \n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame for easier analysis\n",
        "if data and 'data' in data and 'rows' in data['data']:\n",
        "    rows = data['data']['rows']\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "    \n",
        "    print(f\"üìä DataFrame created with {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"üìã Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Show first few rows\n",
        "    print(f\"\\nüîç First 5 repositories:\")\n",
        "    print(df.head())\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available to create DataFrame\")\n",
        "    df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Data Analysis:\n",
            "   Total repositories: 100\n",
            "\n",
            "üîç Missing Values:\n",
            "\n",
            "üåê Programming Languages Distribution:\n",
            "   TypeScript: 27 (27.0%)\n",
            "   Python: 24 (24.0%)\n",
            "   : 12 (12.0%)\n",
            "   Rust: 9 (9.0%)\n",
            "   JavaScript: 7 (7.0%)\n",
            "   Go: 6 (6.0%)\n",
            "   Jupyter Notebook: 4 (4.0%)\n",
            "   Swift: 3 (3.0%)\n",
            "   C++: 2 (2.0%)\n",
            "   Ruby: 1 (1.0%)\n",
            "\n",
            "‚≠ê Stars Statistics:\n",
            "   Min: 10\n",
            "   Max: 1082\n",
            "   Mean: 120.5\n",
            "   Median: 88.0\n",
            "\n",
            "üèÜ Top 10 Repositories by Stars:\n",
            "   ChromeDevTools/chrome-devtools-mcp (TypeScript) - 1082 stars\n",
            "      Chrome DevTools for coding agents\n",
            "\n",
            "   humanlayer/humanlayer (TypeScript) - 621 stars\n",
            "      The best way to get AI coding agents to solve hard problems ...\n",
            "\n",
            "   HKUDS/RAG-Anything (Python) - 398 stars\n",
            "      \"RAG-Anything: All-in-One RAG Framework\"\n",
            "\n",
            "   yasadEv/spyder-osint (Python) - 376 stars\n",
            "      A powerful osint tool.\n",
            "\n",
            "   github/spec-kit (Python) - 351 stars\n",
            "      üí´ Toolkit to help you get started with Spec-Driven Developme...\n",
            "\n",
            "   Gar-b-age/CookLikeHOC (JavaScript) - 309 stars\n",
            "      ü•¢ÂÉèËÄÅ‰π°È∏°üêîÈÇ£Ê†∑ÂÅöÈ•≠„ÄÇ‰∏ªË¶ÅÈÉ®ÂàÜ‰∫é2024Âπ¥ÂÆåÂ∑•ÔºåÈùûËÄÅ‰π°È∏°ÂÆòÊñπ‰ªìÂ∫ì„ÄÇÊñáÂ≠óÊù•Ëá™„ÄäËÄÅ‰π°È∏°ËèúÂìÅÊ∫ØÊ∫êÊä•Âëä„ÄãÔºåÂπ∂ÂÅöÂΩíÁ∫≥„ÄÅÁºñËæë‰∏éÊï¥ÁêÜ„ÄÇ...\n",
            "\n",
            "   subhashchy/The-Accidental-CTO () - 298 stars\n",
            "      How I Scaled from Zero to a Million Store on Dukaan,  Withou...\n",
            "\n",
            "   basecamp/omarchy (Shell) - 261 stars\n",
            "      Opinionated Arch/Hyprland Setup\n",
            "\n",
            "   docusealco/docuseal (Ruby) - 260 stars\n",
            "      Open source DocuSign alternative. Create, fill, and sign dig...\n",
            "\n",
            "   iChochy/NCE (JavaScript) - 246 stars\n",
            "      „ÄäÊñ∞Ê¶ÇÂøµËã±ËØ≠„ÄãÂÖ®ÂõõÂÜåÂú®Á∫øËØæÊñáÊúóËØª„ÄÅÂçïÂè•ÁÇπËØª\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze the data quality and content\n",
        "if df is not None:\n",
        "    print(\"üìà Data Analysis:\")\n",
        "    print(f\"   Total repositories: {len(df)}\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nüîç Missing Values:\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    for col, missing in missing_values.items():\n",
        "        if missing > 0:\n",
        "            print(f\"   {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Analyze primary languages\n",
        "    if 'primary_language' in df.columns:\n",
        "        print(f\"\\nüåê Programming Languages Distribution:\")\n",
        "        lang_counts = df['primary_language'].value_counts().head(10)\n",
        "        for lang, count in lang_counts.items():\n",
        "            print(f\"   {lang}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Analyze stars distribution\n",
        "    if 'stars' in df.columns:\n",
        "        df['stars_numeric'] = pd.to_numeric(df['stars'], errors='coerce')\n",
        "        print(f\"\\n‚≠ê Stars Statistics:\")\n",
        "        print(f\"   Min: {df['stars_numeric'].min()}\")\n",
        "        print(f\"   Max: {df['stars_numeric'].max()}\")\n",
        "        print(f\"   Mean: {df['stars_numeric'].mean():.1f}\")\n",
        "        print(f\"   Median: {df['stars_numeric'].median():.1f}\")\n",
        "    \n",
        "    # Show top repositories by stars\n",
        "    if 'stars_numeric' in df.columns:\n",
        "        print(f\"\\nüèÜ Top 10 Repositories by Stars:\")\n",
        "        top_repos = df.nlargest(10, 'stars_numeric')[['repo_name', 'primary_language', 'stars', 'description']]\n",
        "        for idx, row in top_repos.iterrows():\n",
        "            desc = row['description'][:60] + \"...\" if len(str(row['description'])) > 60 else row['description']\n",
        "            print(f\"   {row['repo_name']} ({row['primary_language']}) - {row['stars']} stars\")\n",
        "            print(f\"      {desc}\")\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Failed to save data: [Errno 2] No such file or directory: '../../data/github_trending_20250928_235749.json'\n",
            "‚ùå Failed to save CSV: Cannot save file into a non-existent directory: '../../data'\n"
          ]
        }
      ],
      "source": [
        "# Save the raw data for further analysis\n",
        "if data:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"../../data/github_trending_{timestamp}.json\"\n",
        "    \n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Raw data saved to: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save data: {e}\")\n",
        "\n",
        "# Save DataFrame as CSV for easy analysis\n",
        "if df is not None:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = f\"../../data/github_trending_{timestamp}.csv\"\n",
        "    \n",
        "    try:\n",
        "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "        print(f\"üíæ DataFrame saved to: {csv_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save CSV: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Filtering for English repositories...\n",
            "   Original count: 100\n",
            "   English count: 87\n",
            "   Filtered out: 13 non-English repos\n",
            "   English percentage: 87.0%\n",
            "\n",
            "üìä English Repositories Analysis:\n",
            "   Total English repos: 87\n",
            "\n",
            "üåê Programming Languages (English repos only):\n",
            "   TypeScript: 23 (26.4%)\n",
            "   Python: 22 (25.3%)\n",
            "   : 10 (11.5%)\n",
            "   Rust: 8 (9.2%)\n",
            "   Go: 5 (5.7%)\n",
            "   JavaScript: 5 (5.7%)\n",
            "   Jupyter Notebook: 4 (4.6%)\n",
            "   Swift: 3 (3.4%)\n",
            "   C++: 2 (2.3%)\n",
            "   Shell: 1 (1.1%)\n",
            "\n",
            "üèÜ Top 10 English Repositories by Stars:\n",
            "   ChromeDevTools/chrome-devtools-mcp (TypeScript) - 1082 stars\n",
            "      Chrome DevTools for coding agents\n",
            "\n",
            "   humanlayer/humanlayer (TypeScript) - 621 stars\n",
            "      The best way to get AI coding agents to solve hard problems in complex codebases...\n",
            "\n",
            "   HKUDS/RAG-Anything (Python) - 398 stars\n",
            "      \"RAG-Anything: All-in-One RAG Framework\"\n",
            "\n",
            "   yasadEv/spyder-osint (Python) - 376 stars\n",
            "      A powerful osint tool.\n",
            "\n",
            "   github/spec-kit (Python) - 351 stars\n",
            "      üí´ Toolkit to help you get started with Spec-Driven Development\n",
            "\n",
            "   subhashchy/The-Accidental-CTO () - 298 stars\n",
            "      How I Scaled from Zero to a Million Store on Dukaan,  Without a CS Degree.  .. A...\n",
            "\n",
            "   basecamp/omarchy (Shell) - 261 stars\n",
            "      Opinionated Arch/Hyprland Setup\n",
            "\n",
            "   docusealco/docuseal (Ruby) - 260 stars\n",
            "      Open source DocuSign alternative. Create, fill, and sign digital documents ‚úçÔ∏è\n",
            "\n",
            "   PicoTrex/Awesome-Nano-Banana-images () - 235 stars\n",
            "      A curated collection of fun and creative examples generated with Nano Bananaüçå, G...\n",
            "\n",
            "   github/copilot-cli () - 224 stars\n",
            "      GitHub Copilot CLI brings the power of Copilot coding agent directly to your ter...\n",
            "\n",
            "‚úÖ Updated main DataFrame to use 87 English repositories\n"
          ]
        }
      ],
      "source": [
        "# English Language Filtering\n",
        "import re\n",
        "from langdetect import detect, LangDetectException\n",
        "\n",
        "def is_english_text(text):\n",
        "    \"\"\"\n",
        "    Check if text is primarily in English\n",
        "    \"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return False\n",
        "    \n",
        "    try:\n",
        "        # Use langdetect to identify language\n",
        "        detected_lang = detect(str(text))\n",
        "        return detected_lang == 'en'\n",
        "    except (LangDetectException, TypeError):\n",
        "        # Fallback: check for common non-English patterns\n",
        "        text_str = str(text).lower()\n",
        "        \n",
        "        # Common non-English indicators\n",
        "        non_english_patterns = [\n",
        "            r'[\\u4e00-\\u9fff]',  # Chinese characters\n",
        "            r'[\\u3040-\\u309f]',  # Hiragana\n",
        "            r'[\\u30a0-\\u30ff]',  # Katakana\n",
        "            r'[\\u0400-\\u04ff]',  # Cyrillic\n",
        "            r'[\\u0600-\\u06ff]',  # Arabic\n",
        "            r'[\\u0590-\\u05ff]',  # Hebrew\n",
        "            r'[\\u0100-\\u017f]',  # Latin Extended (some European languages)\n",
        "        ]\n",
        "        \n",
        "        # Check for non-English patterns\n",
        "        for pattern in non_english_patterns:\n",
        "            if re.search(pattern, text_str):\n",
        "                return False\n",
        "        \n",
        "        # Check for common non-English words/phrases\n",
        "        non_english_indicators = [\n",
        "            '‰∏≠Êñá', '‰∏≠ÂõΩ', 'Êó•Êú¨Ë™û', 'ÌïúÍµ≠Ïñ¥', '—Ä—É—Å—Å–∫–∏–π', 'fran√ßais', 'espa√±ol',\n",
        "            'deutsch', 'italiano', 'portugu√™s', '‰∏≠ÊñáÁâà', 'Êó•Êú¨Ë™ûÁâà', 'ÌïúÍµ≠Ïñ¥Ìåê',\n",
        "            'Âü∫‰∫é', '‰ΩøÁî®', 'ÂºÄÂèë', 'È°πÁõÆ', 'Â∑•ÂÖ∑', 'Ê°ÜÊû∂', 'Á≥ªÁªü', 'Â∫îÁî®',\n",
        "            'ÈñãÁô∫', '„Éó„É≠„Ç∏„Çß„ÇØ„Éà', '„ÉÑ„Éº„É´', '„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ', '„Ç∑„Çπ„ÉÜ„É†', '„Ç¢„Éó„É™',\n",
        "            'Í∞úÎ∞ú', 'ÌîÑÎ°úÏ†ùÌä∏', 'ÎèÑÍµ¨', 'ÌîÑÎ†àÏûÑÏõåÌÅ¨', 'ÏãúÏä§ÌÖú', 'Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò'\n",
        "        ]\n",
        "        \n",
        "        for indicator in non_english_indicators:\n",
        "            if indicator in text_str:\n",
        "                return False\n",
        "        \n",
        "        return True\n",
        "\n",
        "def filter_english_repos(df):\n",
        "    \"\"\"\n",
        "    Filter DataFrame to keep only English repositories\n",
        "    \"\"\"\n",
        "    if df is None:\n",
        "        return None\n",
        "    \n",
        "    print(\"üîç Filtering for English repositories...\")\n",
        "    print(f\"   Original count: {len(df)}\")\n",
        "    \n",
        "    # Apply English filtering to description column\n",
        "    english_mask = df['description'].apply(is_english_text)\n",
        "    english_df = df[english_mask].copy()\n",
        "    \n",
        "    print(f\"   English count: {len(english_df)}\")\n",
        "    print(f\"   Filtered out: {len(df) - len(english_df)} non-English repos\")\n",
        "    print(f\"   English percentage: {len(english_df)/len(df)*100:.1f}%\")\n",
        "    \n",
        "    return english_df\n",
        "\n",
        "# Install langdetect if not available\n",
        "try:\n",
        "    from langdetect import detect\n",
        "except ImportError:\n",
        "    print(\"Installing langdetect...\")\n",
        "    %pip install langdetect\n",
        "    from langdetect import detect\n",
        "\n",
        "# Apply English filtering\n",
        "if df is not None:\n",
        "    english_df = filter_english_repos(df)\n",
        "    \n",
        "    if english_df is not None and len(english_df) > 0:\n",
        "        print(f\"\\nüìä English Repositories Analysis:\")\n",
        "        print(f\"   Total English repos: {len(english_df)}\")\n",
        "        \n",
        "        # Show language distribution for English repos\n",
        "        if 'primary_language' in english_df.columns:\n",
        "            print(f\"\\nüåê Programming Languages (English repos only):\")\n",
        "            lang_counts = english_df['primary_language'].value_counts().head(10)\n",
        "            for lang, count in lang_counts.items():\n",
        "                print(f\"   {lang}: {count} ({count/len(english_df)*100:.1f}%)\")\n",
        "        \n",
        "        # Show top English repositories\n",
        "        if 'stars_numeric' in english_df.columns:\n",
        "            print(f\"\\nüèÜ Top 10 English Repositories by Stars:\")\n",
        "            top_english_repos = english_df.nlargest(10, 'stars_numeric')[['repo_name', 'primary_language', 'stars', 'description']]\n",
        "            for idx, row in top_english_repos.iterrows():\n",
        "                desc = row['description'][:80] + \"...\" if len(str(row['description'])) > 80 else row['description']\n",
        "                print(f\"   {row['repo_name']} ({row['primary_language']}) - {row['stars']} stars\")\n",
        "                print(f\"      {desc}\")\n",
        "                print()\n",
        "        \n",
        "        # Update the main DataFrame to use filtered version\n",
        "        df = english_df\n",
        "        print(f\"‚úÖ Updated main DataFrame to use {len(df)} English repositories\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå No English repositories found or filtering failed\")\n",
        "else:\n",
        "    print(\"‚ùå No data available for English filtering\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Examples of Non-English Repositories (Filtered Out):\n",
            "============================================================\n",
            "üìä Found 14 non-English repositories:\n",
            "\n",
            "‚ùå github/spec-kit (Python) - 351 stars\n",
            "   Description: üí´ Toolkit to help you get started with Spec-Driven Development\n",
            "\n",
            "‚ùå Gar-b-age/CookLikeHOC (JavaScript) - 309 stars\n",
            "   Description: ü•¢ÂÉèËÄÅ‰π°È∏°üêîÈÇ£Ê†∑ÂÅöÈ•≠„ÄÇ‰∏ªË¶ÅÈÉ®ÂàÜ‰∫é2024Âπ¥ÂÆåÂ∑•ÔºåÈùûËÄÅ‰π°È∏°ÂÆòÊñπ‰ªìÂ∫ì„ÄÇÊñáÂ≠óÊù•Ëá™„ÄäËÄÅ‰π°È∏°ËèúÂìÅÊ∫ØÊ∫êÊä•Âëä„ÄãÔºåÂπ∂ÂÅöÂΩíÁ∫≥„ÄÅÁºñËæë‰∏éÊï¥ÁêÜ„ÄÇCookLikeHOC.\n",
            "\n",
            "‚ùå iChochy/NCE (JavaScript) - 246 stars\n",
            "   Description: „ÄäÊñ∞Ê¶ÇÂøµËã±ËØ≠„ÄãÂÖ®ÂõõÂÜåÂú®Á∫øËØæÊñáÊúóËØª„ÄÅÂçïÂè•ÁÇπËØª\n",
            "\n",
            "‚ùå cloudflare/vibesdk (TypeScript) - 147 stars\n",
            "   Description: \n",
            "\n",
            "‚ùå jd-opensource/joyagent-jdgenie (Java) - 125 stars\n",
            "   Description: ÂºÄÊ∫êÁöÑÁ´ØÂà∞Á´Ø‰∫ßÂìÅÁ∫ßÈÄöÁî®Êô∫ËÉΩ‰Ωì\n",
            "\n",
            "üåê Programming Languages (Non-English repos):\n",
            "   TypeScript: 4 (28.6%)\n",
            "   Python: 3 (21.4%)\n",
            "   JavaScript: 2 (14.3%)\n",
            "   : 2 (14.3%)\n",
            "   Java: 1 (7.1%)\n",
            "   Rust: 1 (7.1%)\n",
            "   Go: 1 (7.1%)\n",
            "\n",
            "üìà Filtering Summary:\n",
            "   Total repos: 100\n",
            "   English repos: 86\n",
            "   Non-English repos: 14\n",
            "   English percentage: 86.0%\n"
          ]
        }
      ],
      "source": [
        "# Show Examples of Filtered Out Repositories\n",
        "if df is not None:\n",
        "    # Get the original data before filtering (we need to reload it)\n",
        "    print(\"üîç Examples of Non-English Repositories (Filtered Out):\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Reload original data to show what was filtered\n",
        "    if 'data' in locals() and data and 'data' in data and 'rows' in data['data']:\n",
        "        original_df = pd.DataFrame(data['data']['rows'])\n",
        "        original_df['stars_numeric'] = pd.to_numeric(original_df['stars'], errors='coerce')\n",
        "        \n",
        "        # Apply the same filtering to identify non-English repos\n",
        "        english_mask = original_df['description'].apply(is_english_text)\n",
        "        non_english_df = original_df[~english_mask].copy()\n",
        "        \n",
        "        if len(non_english_df) > 0:\n",
        "            print(f\"üìä Found {len(non_english_df)} non-English repositories:\")\n",
        "            print()\n",
        "            \n",
        "            # Show examples of non-English repos\n",
        "            examples = non_english_df.head(5)\n",
        "            for idx, row in examples.iterrows():\n",
        "                print(f\"‚ùå {row['repo_name']} ({row['primary_language']}) - {row['stars']} stars\")\n",
        "                print(f\"   Description: {row['description']}\")\n",
        "                print()\n",
        "            \n",
        "            # Show language distribution of non-English repos\n",
        "            print(\"üåê Programming Languages (Non-English repos):\")\n",
        "            non_english_lang_counts = non_english_df['primary_language'].value_counts().head(10)\n",
        "            for lang, count in non_english_lang_counts.items():\n",
        "                print(f\"   {lang}: {count} ({count/len(non_english_df)*100:.1f}%)\")\n",
        "            \n",
        "            print(f\"\\nüìà Filtering Summary:\")\n",
        "            print(f\"   Total repos: {len(original_df)}\")\n",
        "            print(f\"   English repos: {len(original_df[english_mask])}\")\n",
        "            print(f\"   Non-English repos: {len(non_english_df)}\")\n",
        "            print(f\"   English percentage: {len(original_df[english_mask])/len(original_df)*100:.1f}%\")\n",
        "            \n",
        "        else:\n",
        "            print(\"‚úÖ All repositories appear to be in English!\")\n",
        "    \n",
        "    else:\n",
        "        print(\"‚ùå Original data not available for comparison\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìñ Fetching README files for top 10 repositories...\n",
            "============================================================\n",
            "üìã [1/10] Fetching README for ChromeDevTools/chrome-devtools-mcp...\n",
            "   ‚úÖ Success - 9,391 characters\n",
            "üìã [2/10] Fetching README for humanlayer/humanlayer...\n",
            "   ‚úÖ Success - 3,951 characters\n",
            "üìã [3/10] Fetching README for HKUDS/RAG-Anything...\n",
            "   ‚úÖ Success - 52,703 characters\n",
            "üìã [4/10] Fetching README for yasadEv/spyder-osint...\n",
            "   ‚ùå Not found\n",
            "üìã [5/10] Fetching README for github/spec-kit...\n",
            "   ‚úÖ Success - 27,858 characters\n",
            "üìã [6/10] Fetching README for subhashchy/The-Accidental-CTO...\n",
            "   ‚úÖ Success - 2,699 characters\n",
            "üìã [7/10] Fetching README for basecamp/omarchy...\n",
            "   ‚úÖ Success - 562 characters\n",
            "üìã [8/10] Fetching README for docusealco/docuseal...\n",
            "   ‚úÖ Success - 5,487 characters\n",
            "üìã [9/10] Fetching README for PicoTrex/Awesome-Nano-Banana-images...\n",
            "   ‚úÖ Success - 60,278 characters\n",
            "üìã [10/10] Fetching README for github/copilot-cli...\n",
            "   ‚úÖ Success - 4,805 characters\n",
            "\n",
            "üìä README Fetching Summary:\n",
            "   Total attempted: 10\n",
            "   Successful: 9\n",
            "   Failed: 1\n",
            "   Success rate: 90.0%\n",
            "\n",
            "üéØ Top 10 Repositories with README Data:\n",
            "======================================================================\n",
            " 1. ‚úÖ ChromeDevTools/chrome-devtools-mcp (TypeScript) - 1082 stars\n",
            "    Description: Chrome DevTools for coding agents\n",
            "    README: # Chrome DevTools MCP  [![npm chrome-devtools-mcp package](https://img.shields.io/npm/v/chrome-devto...\n",
            "\n",
            " 2. ‚úÖ humanlayer/humanlayer (TypeScript) - 621 stars\n",
            "    Description: The best way to get AI coding agents to solve hard problems in complex codebases.\n",
            "    README: <div align=\"center\">  ![Wordmark Logo of HumanLayer](./docs/images/wordmark-light.svg)  </div>  <div...\n",
            "\n",
            " 3. ‚úÖ HKUDS/RAG-Anything (Python) - 398 stars\n",
            "    Description: \"RAG-Anything: All-in-One RAG Framework\"\n",
            "    README: <div align=\"center\">  <div style=\"margin: 20px 0;\">   <img src=\"./assets/logo.png\" width=\"120\" heigh...\n",
            "\n",
            " 4. ‚ùå yasadEv/spyder-osint (Python) - 376 stars\n",
            "    Description: A powerful osint tool.\n",
            "\n",
            " 5. ‚úÖ github/spec-kit (Python) - 351 stars\n",
            "    Description: üí´ Toolkit to help you get started with Spec-Driven Development\n",
            "    README: <div align=\"center\">     <img src=\"./media/logo_small.webp\"/>     <h1>üå± Spec Kit</h1>     <h3><em>Bu...\n",
            "\n",
            " 6. ‚úÖ subhashchy/The-Accidental-CTO () - 298 stars\n",
            "    Description: How I Scaled from Zero to a Million Store on Dukaan,  Without a CS Degree.  .. A System Design Handbook by  Subhash Choudhary \n",
            "    README:  # **The Accidental CTO**  ## **How I Scaled from Zero to a Million Stores on Dukaan, Without a CS D...\n",
            "\n",
            " 7. ‚úÖ basecamp/omarchy (Shell) - 261 stars\n",
            "    Description: Opinionated Arch/Hyprland Setup\n",
            "    README: # Omarchy  Turn a fresh Arch installation into a fully-configured, beautiful, and modern web develop...\n",
            "\n",
            " 8. ‚úÖ docusealco/docuseal (Ruby) - 260 stars\n",
            "    Description: Open source DocuSign alternative. Create, fill, and sign digital documents ‚úçÔ∏è\n",
            "    README: <h1 align=\"center\" style=\"border-bottom: none\">   <div>     <a href=\"https://www.docuseal.com\">     ...\n",
            "\n",
            " 9. ‚úÖ PicoTrex/Awesome-Nano-Banana-images () - 235 stars\n",
            "    Description: A curated collection of fun and creative examples generated with Nano Bananaüçå, Gemini-2.5-flash-image based model. We also release Nano-consistent-150K openly to support the community's development of image generation and unified models(click to website to see our blog)\n",
            "    README: <div align=\"center\">  <img src=\"images/logo.jpg\"  alt=\"ËæìÂÖ•ÂõæÁâá\">   [![License: CC BY 4.0](https://img.s...\n",
            "\n",
            "10. ‚úÖ github/copilot-cli () - 224 stars\n",
            "    Description: GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal. \n",
            "    README: # GitHub Copilot CLI (Public Preview)  The power of GitHub Copilot, now in your terminal.  GitHub Co...\n",
            "\n",
            "‚úÖ DataFrame ready with README content for 10 repositories\n"
          ]
        }
      ],
      "source": [
        "# Fetch README Files for Top 10 Repositories\n",
        "def fetch_readme_content(repo_name, timeout=10):\n",
        "    \"\"\"\n",
        "    Fetch README content from GitHub repository\n",
        "    \"\"\"\n",
        "    # Try different common README file names and branch combinations\n",
        "    readme_variants = [\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/main/README.md\",\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/master/README.md\",\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/main/readme.md\",\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/master/readme.md\",\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/main/README.rst\",\n",
        "        f\"https://raw.githubusercontent.com/{repo_name}/master/README.rst\",\n",
        "    ]\n",
        "    \n",
        "    for readme_url in readme_variants:\n",
        "        try:\n",
        "            response = requests.get(readme_url, timeout=timeout)\n",
        "            if response.status_code == 200:\n",
        "                content = response.text\n",
        "                # Check if content is not empty and looks like a README\n",
        "                if content.strip() and len(content) > 50:\n",
        "                    return {\n",
        "                        'readme_url': readme_url,\n",
        "                        'readme_content': content,\n",
        "                        'readme_length': len(content),\n",
        "                        'status': 'success'\n",
        "                    }\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    return {\n",
        "        'readme_url': None,\n",
        "        'readme_content': None,\n",
        "        'readme_length': 0,\n",
        "        'status': 'not_found'\n",
        "    }\n",
        "\n",
        "def add_readme_to_dataframe(df, top_n=10):\n",
        "    \"\"\"\n",
        "    Add README content to DataFrame for top N repositories\n",
        "    \"\"\"\n",
        "    if df is None or len(df) == 0:\n",
        "        print(\"‚ùå No data available for README fetching\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üìñ Fetching README files for top {top_n} repositories...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Get top N repositories by stars\n",
        "    top_repos = df.nlargest(top_n, 'stars_numeric').copy()\n",
        "    \n",
        "    readme_data = []\n",
        "    \n",
        "    for idx, row in top_repos.iterrows():\n",
        "        repo_name = row['repo_name']\n",
        "        print(f\"üìã [{len(readme_data)+1}/{top_n}] Fetching README for {repo_name}...\")\n",
        "        \n",
        "        # Fetch README content\n",
        "        readme_info = fetch_readme_content(repo_name)\n",
        "        readme_data.append(readme_info)\n",
        "        \n",
        "        # Display status\n",
        "        if readme_info['status'] == 'success':\n",
        "            print(f\"   ‚úÖ Success - {readme_info['readme_length']:,} characters\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Not found\")\n",
        "        \n",
        "        # Small delay to be respectful to GitHub\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    # Add README data to the top repos DataFrame\n",
        "    readme_df = pd.DataFrame(readme_data)\n",
        "    top_repos_with_readme = pd.concat([top_repos.reset_index(drop=True), readme_df], axis=1)\n",
        "    \n",
        "    # Summary\n",
        "    successful_readmes = len([r for r in readme_data if r['status'] == 'success'])\n",
        "    print(f\"\\nüìä README Fetching Summary:\")\n",
        "    print(f\"   Total attempted: {len(readme_data)}\")\n",
        "    print(f\"   Successful: {successful_readmes}\")\n",
        "    print(f\"   Failed: {len(readme_data) - successful_readmes}\")\n",
        "    print(f\"   Success rate: {successful_readmes/len(readme_data)*100:.1f}%\")\n",
        "    \n",
        "    return top_repos_with_readme\n",
        "\n",
        "# Fetch README files for top 10 English repositories\n",
        "if df is not None and len(df) > 0:\n",
        "    top_10_with_readme = add_readme_to_dataframe(df, top_n=10)\n",
        "    \n",
        "    if top_10_with_readme is not None:\n",
        "        print(f\"\\nüéØ Top 10 Repositories with README Data:\")\n",
        "        print(\"=\" * 70)\n",
        "        \n",
        "        for idx, row in top_10_with_readme.iterrows():\n",
        "            status_emoji = \"‚úÖ\" if row['status'] == 'success' else \"‚ùå\"\n",
        "            readme_preview = \"\"\n",
        "            \n",
        "            if row['status'] == 'success' and row['readme_content']:\n",
        "                # Show first 100 characters of README\n",
        "                readme_preview = str(row['readme_content'])[:100].replace('\\n', ' ')\n",
        "                if len(str(row['readme_content'])) > 100:\n",
        "                    readme_preview += \"...\"\n",
        "            \n",
        "            print(f\"{idx+1:2d}. {status_emoji} {row['repo_name']} ({row['primary_language']}) - {row['stars']} stars\")\n",
        "            print(f\"    Description: {row['description']}\")\n",
        "            if readme_preview:\n",
        "                print(f\"    README: {readme_preview}\")\n",
        "            print()\n",
        "        \n",
        "        # Update the main DataFrame reference\n",
        "        df_with_readme = top_10_with_readme\n",
        "        print(f\"‚úÖ DataFrame ready with README content for {len(df_with_readme)} repositories\")\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå Failed to fetch README data\")\n",
        "        df_with_readme = None\n",
        "else:\n",
        "    print(\"‚ùå No data available for README fetching\")\n",
        "    df_with_readme = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We've Discovered:\n",
        "- ‚úÖ Successfully connected to OSS Insight API\n",
        "- ‚úÖ Retrieved trending repositories from past 24 hours\n",
        "- ‚úÖ Analyzed data structure and quality\n",
        "- ‚úÖ Saved data for further analysis\n",
        "\n",
        "### Potential Integration with Newsletter:\n",
        "1. **Content Source**: Trending GitHub projects could be a valuable addition to tech newsletters\n",
        "2. **Data Quality**: High-quality data with stars, descriptions, and metadata\n",
        "3. **Real-time**: Past 24 hours data provides fresh, relevant content\n",
        "4. **Diverse Languages**: Covers projects in multiple programming languages\n",
        "\n",
        "### Next Research Areas:\n",
        "- Filter for tech-relevant projects (AI, web development, etc.)\n",
        "- Analyze project descriptions for quality scoring\n",
        "- Explore integration with existing pipeline\n",
        "- Test different time periods and language filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Repository Ranking Agent initialized\n",
            "   Ollama Host: 172.22.128.1\n",
            "   Model: llama3.2:3b\n"
          ]
        }
      ],
      "source": [
        "# LLM Agent for Repository Ranking\n",
        "import requests\n",
        "import json\n",
        "\n",
        "class RepositoryRankingAgent:\n",
        "    def __init__(self, ollama_host=\"172.22.128.1\", model=\"llama3.2:3b\"):\n",
        "        self.ollama_host = ollama_host\n",
        "        self.model = model\n",
        "        self.api_url = f\"http://{ollama_host}:11434/api/generate\"\n",
        "        \n",
        "    def create_ranking_prompt(self, repositories):\n",
        "        \"\"\"\n",
        "        Create a prompt for ranking repositories based on impact and innovation\n",
        "        \"\"\"\n",
        "        prompt = \"\"\"You are an expert tech analyst. Your task is to rank the provided GitHub repositories from 1‚Äì10 based on their potential impact and innovation.\n",
        "\n",
        "Ranking criteria:\n",
        "1. Innovation & Technical Merit: How novel or technically impressive is the project?\n",
        "2. Potential Impact: How likely is this to influence the developer community or industry?\n",
        "3. Practical Value: How useful would this be for developers?\n",
        "4. Code Quality Indicators: Based on README quality and project description.\n",
        "5. Trending Potential: How likely is this to continue growing in popularity?\n",
        "\n",
        "STRICT OUTPUT RULES:\n",
        "- Respond with ONLY a valid JSON object.\n",
        "- Do not include markdown code blocks, explanations, comments, or extra text.\n",
        "- Each repo must have a unique rank from 1‚Äì10.\n",
        "- Use repo_name exactly as provided (no modifications).\n",
        "- Each reason must be ‚â§25 words and reference one or more ranking criteria.\n",
        "- overall_analysis must be ‚â§50 words summarizing observed quality and trends.\n",
        "\n",
        "Required JSON format:\n",
        "{\n",
        "  \"rankings\": [\n",
        "    {\"rank\": 1, \"repo_name\": \"exact_repo_name\", \"reason\": \"brief explanation\"},\n",
        "    ...\n",
        "    {\"rank\": 10, \"repo_name\": \"exact_repo_name\", \"reason\": \"brief explanation\"}\n",
        "  ],\n",
        "  \"overall_analysis\": \"Brief summary of overall quality and trends\"\n",
        "}\n",
        "\n",
        "Repositories to rank:\n",
        "\n",
        "\"\"\"\n",
        "        \n",
        "        for i, repo in enumerate(repositories, 1):\n",
        "            prompt += f\"{i}. **{repo['repo_name']}** ({repo['primary_language']}) - {repo['stars']} stars\\n\"\n",
        "            prompt += f\"   Description: {repo['description']}\\n\"\n",
        "            \n",
        "            if repo.get('readme_preview'):\n",
        "                prompt += f\"   README: {repo['readme_preview']}\\n\"\n",
        "            else:\n",
        "                prompt += f\"   README: Not available\\n\"\n",
        "            prompt += \"\\n\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def get_llm_ranking(self, repositories):\n",
        "        \"\"\"\n",
        "        Get ranking from LLM via Ollama\n",
        "        \"\"\"\n",
        "        prompt = self.create_ranking_prompt(repositories)\n",
        "        \n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.3,  # Lower temperature for more consistent rankings\n",
        "                \"top_p\": 0.9,\n",
        "                \"max_tokens\": 2000\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            print(f\"ü§ñ Sending request to Ollama at {self.ollama_host}...\")\n",
        "            response = requests.post(self.api_url, json=payload, timeout=60)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                llm_response = result.get('response', '')\n",
        "                print(f\"‚úÖ LLM response received ({len(llm_response)} characters)\")\n",
        "                return llm_response\n",
        "            else:\n",
        "                print(f\"‚ùå Ollama API error: {response.status_code} - {response.text}\")\n",
        "                return None\n",
        "                \n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"‚ùå Connection error to Ollama: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Unexpected error: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def parse_llm_response(self, response_text):\n",
        "        \"\"\"\n",
        "        Parse LLM response and extract JSON ranking\n",
        "        \"\"\"\n",
        "        if not response_text:\n",
        "            return None\n",
        "        \n",
        "        # Clean the response text\n",
        "        response_text = response_text.strip()\n",
        "        \n",
        "        try:\n",
        "            # Method 1: Try to parse the entire response as JSON\n",
        "            try:\n",
        "                ranking_data = json.loads(response_text)\n",
        "                return ranking_data\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "            # Method 2: Try to find JSON object in the response\n",
        "            start_idx = response_text.find('{')\n",
        "            end_idx = response_text.rfind('}') + 1\n",
        "            \n",
        "            if start_idx != -1 and end_idx > start_idx:\n",
        "                json_str = response_text[start_idx:end_idx]\n",
        "                ranking_data = json.loads(json_str)\n",
        "                return ranking_data\n",
        "            \n",
        "            # Method 3: Try to extract from markdown code blocks\n",
        "            if '```json' in response_text:\n",
        "                start = response_text.find('```json') + 7\n",
        "                end = response_text.find('```', start)\n",
        "                if end > start:\n",
        "                    json_str = response_text[start:end].strip()\n",
        "                    ranking_data = json.loads(json_str)\n",
        "                    return ranking_data\n",
        "            \n",
        "            # Method 4: Try to extract from code blocks without json marker\n",
        "            if '```' in response_text:\n",
        "                parts = response_text.split('```')\n",
        "                for part in parts:\n",
        "                    part = part.strip()\n",
        "                    if part.startswith('{') and part.endswith('}'):\n",
        "                        try:\n",
        "                            ranking_data = json.loads(part)\n",
        "                            return ranking_data\n",
        "                        except:\n",
        "                            continue\n",
        "            \n",
        "            print(\"‚ö†Ô∏è  No valid JSON found in LLM response\")\n",
        "            print(f\"Response preview: {response_text[:300]}...\")\n",
        "            return None\n",
        "                \n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è  JSON parsing error: {e}\")\n",
        "            print(f\"Raw response: {response_text[:500]}...\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Unexpected parsing error: {e}\")\n",
        "            print(f\"Raw response: {response_text[:500]}...\")\n",
        "            return None\n",
        "    \n",
        "    def parse_fallback_ranking(self, response_text, repositories):\n",
        "        \"\"\"\n",
        "        Fallback method to extract ranking from non-JSON response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Attempting fallback parsing...\")\n",
        "            \n",
        "            # Extract repository names from the response\n",
        "            repo_names = []\n",
        "            lines = response_text.split('\\n')\n",
        "            \n",
        "            for line in lines:\n",
        "                line = line.strip()\n",
        "                # Look for patterns like \"1. repo_name\" or \"**repo_name**\"\n",
        "                if any(repo['repo_name'] in line for repo in repositories):\n",
        "                    for repo in repositories:\n",
        "                        if repo['repo_name'] in line:\n",
        "                            repo_names.append(repo['repo_name'])\n",
        "                            break\n",
        "            \n",
        "            if repo_names:\n",
        "                # Create a simple ranking structure\n",
        "                rankings = []\n",
        "                for i, repo_name in enumerate(repo_names[:10], 1):\n",
        "                    rankings.append({\n",
        "                        \"rank\": i,\n",
        "                        \"repo_name\": repo_name,\n",
        "                        \"reason\": f\"Ranked by LLM based on impact and innovation\"\n",
        "                    })\n",
        "                \n",
        "                return {\n",
        "                    \"rankings\": rankings,\n",
        "                    \"overall_analysis\": \"Fallback ranking extracted from LLM response\"\n",
        "                }\n",
        "            \n",
        "            return None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Fallback parsing failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# Initialize the ranking agent\n",
        "ranking_agent = RepositoryRankingAgent()\n",
        "print(f\"ü§ñ Repository Ranking Agent initialized\")\n",
        "print(f\"   Ollama Host: {ranking_agent.ollama_host}\")\n",
        "print(f\"   Model: {ranking_agent.model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Getting LLM Ranking for Top 10 Repositories...\n",
            "============================================================\n",
            "üìä Prepared data for 10 repositories\n",
            "\n",
            "üìà Current API Ranking (by stars):\n",
            "    1. üìñ ChromeDevTools/chrome-devtools-mcp (TypeScript) - 1082 stars\n",
            "    2. üìñ humanlayer/humanlayer (TypeScript) - 621 stars\n",
            "    3. üìñ HKUDS/RAG-Anything (Python) - 398 stars\n",
            "    4. ‚ùå yasadEv/spyder-osint (Python) - 376 stars\n",
            "    5. üìñ github/spec-kit (Python) - 351 stars\n",
            "    6. üìñ subhashchy/The-Accidental-CTO () - 298 stars\n",
            "    7. üìñ basecamp/omarchy (Shell) - 261 stars\n",
            "    8. üìñ docusealco/docuseal (Ruby) - 260 stars\n",
            "    9. üìñ PicoTrex/Awesome-Nano-Banana-images () - 235 stars\n",
            "   10. üìñ github/copilot-cli () - 224 stars\n",
            "ü§ñ Sending request to Ollama at 172.22.128.1...\n",
            "‚úÖ LLM response received (1442 characters)\n",
            "\n",
            "ü§ñ LLM Response Preview:\n",
            "   Here is the final answer in the required format:\n",
            "\n",
            "**Ranking of Open Source Projects by Star Count**\n",
            "\n",
            "1. **github/copilot-cli**: 224 stars\n",
            "2. **basecamp/omarchy**: 261 stars\n",
            "3. **docusealco/docuseal**:...\n",
            "‚ö†Ô∏è  No valid JSON found in LLM response\n",
            "Response preview: Here is the final answer in the required format:\n",
            "\n",
            "**Ranking of Open Source Projects by Star Count**\n",
            "\n",
            "1. **github/copilot-cli**: 224 stars\n",
            "2. **basecamp/omarchy**: 261 stars\n",
            "3. **docusealco/docuseal**: 260 stars\n",
            "4. **PicoTrex/Awesome-Nano-Banana-images**: 235 stars\n",
            "5. **subhashchy/The-Accidental-CTO*...\n",
            "üîÑ Attempting fallback parsing...\n",
            "\n",
            "üèÜ LLM Ranking Results:\n",
            "============================================================\n",
            "    1. github/copilot-cli () - 224 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    2. basecamp/omarchy (Shell) - 261 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    3. docusealco/docuseal (Ruby) - 260 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    4. PicoTrex/Awesome-Nano-Banana-images () - 235 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    5. subhashchy/The-Accidental-CTO () - 298 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    6. docusealco/docuseal (Ruby) - 260 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    7. PicoTrex/Awesome-Nano-Banana-images () - 235 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    8. github/copilot-cli () - 224 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "    9. basecamp/omarchy (Shell) - 261 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "   10. docusealco/docuseal (Ruby) - 260 stars\n",
            "       Reason: Ranked by LLM based on impact and innovation\n",
            "\n",
            "üìä Overall Analysis:\n",
            "   Fallback ranking extracted from LLM response\n",
            "\n",
            "üîÑ Comparison: API vs LLM Ranking\n",
            "----------------------------------------\n",
            "API Top 3:    ChromeDevTools/chrome-devtools-mcp, humanlayer/humanlayer, HKUDS/RAG-Anything\n",
            "LLM Top 3:    github/copilot-cli, basecamp/omarchy, docusealco/docuseal\n",
            "Common:       None\n",
            "API only:     humanlayer/humanlayer, HKUDS/RAG-Anything, ChromeDevTools/chrome-devtools-mcp\n",
            "LLM only:     github/copilot-cli, basecamp/omarchy, docusealco/docuseal\n",
            "\n",
            "‚úÖ Ranking analysis complete!\n"
          ]
        }
      ],
      "source": [
        "# Get LLM Ranking for Top 10 Repositories\n",
        "if 'df_with_readme' in locals() and df_with_readme is not None:\n",
        "    print(\"üéØ Getting LLM Ranking for Top 10 Repositories...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Prepare repository data for LLM\n",
        "    repo_data_for_llm = []\n",
        "    \n",
        "    for idx, row in df_with_readme.iterrows():\n",
        "        repo_info = {\n",
        "            'repo_name': row['repo_name'],\n",
        "            'primary_language': row['primary_language'],\n",
        "            'stars': int(row['stars_numeric']),\n",
        "            'description': row['description'],\n",
        "            'readme_preview': None\n",
        "        }\n",
        "        \n",
        "        # Add README preview if available\n",
        "        if row['status'] == 'success' and row['readme_content']:\n",
        "            readme_text = str(row['readme_content'])\n",
        "            # Get first 100 tokens (approximate)\n",
        "            words = readme_text.split()[:100]\n",
        "            repo_info['readme_preview'] = ' '.join(words)\n",
        "        \n",
        "        repo_data_for_llm.append(repo_info)\n",
        "    \n",
        "    print(f\"üìä Prepared data for {len(repo_data_for_llm)} repositories\")\n",
        "    \n",
        "    # Show current API ranking (by stars)\n",
        "    print(f\"\\nüìà Current API Ranking (by stars):\")\n",
        "    for i, repo in enumerate(repo_data_for_llm, 1):\n",
        "        readme_status = \"üìñ\" if repo['readme_preview'] else \"‚ùå\"\n",
        "        print(f\"   {i:2d}. {readme_status} {repo['repo_name']} ({repo['primary_language']}) - {repo['stars']} stars\")\n",
        "    \n",
        "    # Get LLM ranking\n",
        "    llm_response = ranking_agent.get_llm_ranking(repo_data_for_llm)\n",
        "    \n",
        "    if llm_response:\n",
        "        print(f\"\\nü§ñ LLM Response Preview:\")\n",
        "        print(f\"   {llm_response[:200]}...\")\n",
        "        \n",
        "        # Parse the response\n",
        "        llm_ranking = ranking_agent.parse_llm_response(llm_response)\n",
        "        \n",
        "        # If JSON parsing fails, try fallback parsing\n",
        "        if not llm_ranking:\n",
        "            llm_ranking = ranking_agent.parse_fallback_ranking(llm_response, repo_data_for_llm)\n",
        "        \n",
        "        if llm_ranking:\n",
        "            print(f\"\\nüèÜ LLM Ranking Results:\")\n",
        "            print(\"=\" * 60)\n",
        "            \n",
        "            # Display LLM rankings\n",
        "            for ranking in llm_ranking.get('rankings', []):\n",
        "                rank = ranking.get('rank', 'N/A')\n",
        "                repo_name = ranking.get('repo_name', 'Unknown')\n",
        "                reason = ranking.get('reason', 'No reason provided')\n",
        "                \n",
        "                # Find original repo info\n",
        "                original_repo = next((r for r in repo_data_for_llm if r['repo_name'] == repo_name), None)\n",
        "                if original_repo:\n",
        "                    stars = original_repo['stars']\n",
        "                    lang = original_repo['primary_language']\n",
        "                    print(f\"   {rank:2d}. {repo_name} ({lang}) - {stars} stars\")\n",
        "                    print(f\"       Reason: {reason}\")\n",
        "                else:\n",
        "                    print(f\"   {rank:2d}. {repo_name}\")\n",
        "                    print(f\"       Reason: {reason}\")\n",
        "                print()\n",
        "            \n",
        "            # Show overall analysis\n",
        "            overall_analysis = llm_ranking.get('overall_analysis', '')\n",
        "            if overall_analysis:\n",
        "                print(f\"üìä Overall Analysis:\")\n",
        "                print(f\"   {overall_analysis}\")\n",
        "            \n",
        "            # Compare with API ranking\n",
        "            print(f\"\\nüîÑ Comparison: API vs LLM Ranking\")\n",
        "            print(\"-\" * 40)\n",
        "            \n",
        "            api_top_3 = [r['repo_name'] for r in repo_data_for_llm[:3]]\n",
        "            llm_top_3 = [r['repo_name'] for r in llm_ranking.get('rankings', [])[:3]]\n",
        "            \n",
        "            print(f\"API Top 3:    {', '.join(api_top_3)}\")\n",
        "            print(f\"LLM Top 3:    {', '.join(llm_top_3)}\")\n",
        "            \n",
        "            # Check for differences\n",
        "            api_set = set(api_top_3)\n",
        "            llm_set = set(llm_top_3)\n",
        "            common = api_set.intersection(llm_set)\n",
        "            api_only = api_set - llm_set\n",
        "            llm_only = llm_set - api_set\n",
        "            \n",
        "            print(f\"Common:       {', '.join(common) if common else 'None'}\")\n",
        "            print(f\"API only:     {', '.join(api_only) if api_only else 'None'}\")\n",
        "            print(f\"LLM only:     {', '.join(llm_only) if llm_only else 'None'}\")\n",
        "            \n",
        "            # Store LLM ranking for potential use\n",
        "            llm_ranking_data = llm_ranking\n",
        "            \n",
        "        else:\n",
        "            print(\"‚ùå Failed to parse LLM ranking - using API ranking\")\n",
        "            llm_ranking_data = None\n",
        "    \n",
        "    else:\n",
        "        print(\"‚ùå Failed to get LLM ranking - using API ranking\")\n",
        "        llm_ranking_data = None\n",
        "    \n",
        "    print(f\"\\n‚úÖ Ranking analysis complete!\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No repository data available for LLM ranking\")\n",
        "    llm_ranking_data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Final Status:\n",
            "   ‚úÖ LLM ranking available with 10 repositories\n"
          ]
        }
      ],
      "source": [
        "# Alternative Simple Prompt for LLM (if the main one fails)\n",
        "def create_simple_prompt(repositories):\n",
        "    \"\"\"\n",
        "    Create a simpler prompt that might work better with smaller models\n",
        "    \"\"\"\n",
        "    prompt = \"\"\"Rank these GitHub repositories 1-10 by innovation and impact. Respond with only JSON:\n",
        "\n",
        "{\n",
        "  \"rankings\": [\n",
        "    {\"rank\": 1, \"repo_name\": \"repo_name\", \"reason\": \"brief reason\"},\n",
        "    {\"rank\": 2, \"repo_name\": \"repo_name\", \"reason\": \"brief reason\"}\n",
        "  ]\n",
        "}\n",
        "\n",
        "Repositories:\n",
        "\"\"\"\n",
        "    \n",
        "    for i, repo in enumerate(repositories, 1):\n",
        "        prompt += f\"{i}. {repo['repo_name']} ({repo['primary_language']}) - {repo['stars']} stars\\n\"\n",
        "        prompt += f\"   {repo['description']}\\n\"\n",
        "        if repo.get('readme_preview'):\n",
        "            prompt += f\"   README: {repo['readme_preview'][:200]}...\\n\"\n",
        "        prompt += \"\\n\"\n",
        "    \n",
        "    return prompt\n",
        "\n",
        "# Test with simpler prompt if needed\n",
        "if 'llm_ranking_data' in locals() and llm_ranking_data is None:\n",
        "    print(\"\\nüîÑ Trying simpler prompt approach...\")\n",
        "    \n",
        "    simple_prompt = create_simple_prompt(repo_data_for_llm)\n",
        "    \n",
        "    # Send simpler prompt to LLM\n",
        "    simple_payload = {\n",
        "        \"model\": \"llama3.2:3b\",\n",
        "        \"prompt\": simple_prompt,\n",
        "        \"stream\": False,\n",
        "        \"options\": {\n",
        "            \"temperature\": 0.1,  # Very low temperature for consistent output\n",
        "            \"max_tokens\": 1000\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        print(\"ü§ñ Sending simple prompt to Ollama...\")\n",
        "        response = requests.post(ranking_agent.api_url, json=simple_payload, timeout=60)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            simple_response = result.get('response', '')\n",
        "            print(f\"‚úÖ Simple prompt response received\")\n",
        "            \n",
        "            # Try to parse the simple response\n",
        "            simple_ranking = ranking_agent.parse_llm_response(simple_response)\n",
        "            if simple_ranking:\n",
        "                print(\"‚úÖ Successfully parsed simple prompt response!\")\n",
        "                llm_ranking_data = simple_ranking\n",
        "                \n",
        "                # Display the simple ranking\n",
        "                print(f\"\\nüèÜ Simple LLM Ranking:\")\n",
        "                for ranking in simple_ranking.get('rankings', []):\n",
        "                    rank = ranking.get('rank', 'N/A')\n",
        "                    repo_name = ranking.get('repo_name', 'Unknown')\n",
        "                    reason = ranking.get('reason', 'No reason')\n",
        "                    print(f\"   {rank}. {repo_name} - {reason}\")\n",
        "            else:\n",
        "                print(\"‚ùå Simple prompt also failed to parse\")\n",
        "        else:\n",
        "            print(f\"‚ùå Simple prompt request failed: {response.status_code}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Simple prompt error: {e}\")\n",
        "\n",
        "print(f\"\\nüìä Final Status:\")\n",
        "if 'llm_ranking_data' in locals() and llm_ranking_data:\n",
        "    print(f\"   ‚úÖ LLM ranking available with {len(llm_ranking_data.get('rankings', []))} repositories\")\n",
        "else:\n",
        "    print(f\"   ‚ùå Using API ranking only (LLM ranking failed)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Repository Description Agent initialized\n",
            "   Ollama Host: 172.22.128.1\n",
            "   Model: llama3.2:3b\n"
          ]
        }
      ],
      "source": [
        "# Repository Description Agent\n",
        "class RepositoryDescriptionAgent:\n",
        "    def __init__(self, ollama_host=\"172.22.128.1\", model=\"llama3.2:3b\"):\n",
        "        self.ollama_host = ollama_host\n",
        "        self.model = model\n",
        "        self.api_url = f\"http://{ollama_host}:11434/api/generate\"\n",
        "        self.fallback_repo = {\n",
        "            \"repo_name\": \"llamasoft/useless\",\n",
        "            \"github_url\": \"https://github.com/llamasoft/useless\",\n",
        "            \"description\": \"Our content pipeline encountered technical difficulties. We're working to restore normal service and will have fresh trending repositories for you soon.\"\n",
        "        }\n",
        "        \n",
        "    def create_description_prompt(self, repo_info):\n",
        "        \"\"\"\n",
        "        Create a prompt for generating repository descriptions\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"You are a technical writer for a developer-focused newsletter.  \n",
        "Write a 1‚Äì2 sentence description of the given GitHub repository.  \n",
        "\n",
        "Requirements:\n",
        "- Maximum 25 words.  \n",
        "- Simple, clear, and jargon-free.  \n",
        "- Focus only on what the project does.  \n",
        "- No hype, no extra details.  \n",
        "\n",
        "Input: {repo_info['repo_name']} - {repo_info['description']}\n",
        "\n",
        "Output: Plain text, 1‚Äì2 sentences.\"\"\"\n",
        "        \n",
        "        return prompt\n",
        "    \n",
        "    def generate_description(self, repo_info):\n",
        "        \"\"\"\n",
        "        Generate description for a single repository\n",
        "        \"\"\"\n",
        "        prompt = self.create_description_prompt(repo_info)\n",
        "        \n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.3,\n",
        "                \"max_tokens\": 150  # Keep it short\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(self.api_url, json=payload, timeout=30)\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                description = result.get('response', '').strip()\n",
        "                \n",
        "                # Clean up the description\n",
        "                if description:\n",
        "                    # Remove any markdown formatting\n",
        "                    description = description.replace('**', '').replace('*', '').replace('`', '')\n",
        "                    # Remove extra whitespace\n",
        "                    description = ' '.join(description.split())\n",
        "                    return description\n",
        "            \n",
        "            return None\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Description generation failed for {repo_info['repo_name']}: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def process_repositories(self, repositories):\n",
        "        \"\"\"\n",
        "        Process repositories until we get one successful result, then stop\n",
        "        \"\"\"\n",
        "        processed_repos = []\n",
        "        \n",
        "        print(f\"üìù Processing repositories until first success...\")\n",
        "        \n",
        "        for i, repo in enumerate(repositories, 1):\n",
        "            print(f\"üìã [{i}/{len(repositories)}] Processing {repo['repo_name']}...\")\n",
        "            \n",
        "            # Try to generate description\n",
        "            summary = self.generate_description(repo)\n",
        "            \n",
        "            if summary:\n",
        "                repo_info = {\n",
        "                    \"rank\": 1,  # Always rank 1 since we only take the first success\n",
        "                    \"repo_name\": repo['repo_name'],\n",
        "                    \"primary_language\": repo.get('primary_language', ''),\n",
        "                    \"stars\": repo.get('stars', 0),\n",
        "                    \"github_url\": f\"https://github.com/{repo['repo_name']}\",\n",
        "                    \"original_description\": repo['description'],\n",
        "                    \"summary\": summary,\n",
        "                    \"status\": \"success\"\n",
        "                }\n",
        "                processed_repos.append(repo_info)\n",
        "                print(f\"   ‚úÖ Generated: {summary[:80]}...\")\n",
        "                print(f\"   üéØ Success! Stopping here.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"   ‚ùå Failed, trying next repository...\")\n",
        "        \n",
        "        # If no successful descriptions, use fallback\n",
        "        if not processed_repos:\n",
        "            print(f\"   üîÑ No successful descriptions, using fallback repository\")\n",
        "            fallback_repo = self.fallback_repo.copy()\n",
        "            fallback_repo.update({\n",
        "                \"rank\": 1,\n",
        "                \"primary_language\": \"Unknown\",\n",
        "                \"stars\": 0,\n",
        "                \"original_description\": \"No repositories processed successfully\",\n",
        "                \"status\": \"fallback\"\n",
        "            })\n",
        "            processed_repos.append(fallback_repo)\n",
        "        \n",
        "        successful_count = len([r for r in processed_repos if r['status'] == 'success'])\n",
        "        \n",
        "        print(f\"\\nüìä Description Generation Summary:\")\n",
        "        print(f\"   Attempted repositories: {len(processed_repos)}\")\n",
        "        print(f\"   Successful descriptions: {successful_count}\")\n",
        "        print(f\"   Final result: {'Success' if successful_count > 0 else 'Fallback used'}\")\n",
        "        \n",
        "        return processed_repos\n",
        "\n",
        "# Initialize the description agent\n",
        "description_agent = RepositoryDescriptionAgent()\n",
        "print(f\"üìù Repository Description Agent initialized\")\n",
        "print(f\"   Ollama Host: {description_agent.ollama_host}\")\n",
        "print(f\"   Model: {description_agent.model}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì∞ Generating Newsletter Descriptions...\n",
            "==================================================\n",
            "üéØ Using LLM ranking for repository order\n",
            "üìã Processing top 5 repositories (LLM ranking)\n",
            "üìù Processing repositories until first success...\n",
            "üìã [1/5] Processing github/copilot-cli...\n",
            "   ‚úÖ Generated: The GitHub repository for Copilot CLI provides a command-line interface to lever...\n",
            "   üéØ Success! Stopping here.\n",
            "\n",
            "üìä Description Generation Summary:\n",
            "   Attempted repositories: 1\n",
            "   Successful descriptions: 1\n",
            "   Final result: Success\n",
            "\n",
            "üì∞ Newsletter Content Preview:\n",
            "============================================================\n",
            "1. **github/copilot-cli** () - 224 stars\n",
            "   üîó https://github.com/github/copilot-cli\n",
            "   üìù The GitHub repository for Copilot CLI provides a command-line interface to leverage Copilot's coding capabilities directly in the terminal.\n",
            "\n",
            "‚úÖ Newsletter content generated successfully!\n",
            "   Ranking source: LLM\n",
            "   Total repositories: 1\n",
            "   Successful descriptions: 1\n"
          ]
        }
      ],
      "source": [
        "# Generate Newsletter Descriptions for Repositories\n",
        "if 'repo_data_for_llm' in locals() and repo_data_for_llm:\n",
        "    print(\"üì∞ Generating Newsletter Descriptions...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Determine which ranking to use\n",
        "    if 'llm_ranking_data' in locals() and llm_ranking_data and llm_ranking_data.get('rankings'):\n",
        "        print(\"üéØ Using LLM ranking for repository order\")\n",
        "        # Use LLM ranking order\n",
        "        ranked_repos = []\n",
        "        for ranking in llm_ranking_data['rankings']:\n",
        "            repo_name = ranking['repo_name']\n",
        "            # Find the repo data\n",
        "            original_repo = next((r for r in repo_data_for_llm if r['repo_name'] == repo_name), None)\n",
        "            if original_repo:\n",
        "                ranked_repos.append(original_repo)\n",
        "        \n",
        "        # Add any repos that weren't in the LLM ranking\n",
        "        for repo in repo_data_for_llm:\n",
        "            if repo not in ranked_repos:\n",
        "                ranked_repos.append(repo)\n",
        "                \n",
        "        repositories_to_process = ranked_repos[:5]  # Top 5 for newsletter\n",
        "        ranking_source = \"LLM\"\n",
        "        \n",
        "    else:\n",
        "        print(\"üìä Using API ranking (by stars) for repository order\")\n",
        "        repositories_to_process = repo_data_for_llm[:5]  # Top 5 by stars\n",
        "        ranking_source = \"API\"\n",
        "    \n",
        "    print(f\"üìã Processing top {len(repositories_to_process)} repositories ({ranking_source} ranking)\")\n",
        "    \n",
        "    # Generate descriptions\n",
        "    newsletter_repos = description_agent.process_repositories(repositories_to_process)\n",
        "    \n",
        "    print(f\"\\nüì∞ Newsletter Content Preview:\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for repo in newsletter_repos:\n",
        "        print(f\"{repo['rank']}. **{repo['repo_name']}** ({repo.get('primary_language', 'Unknown')}) - {repo.get('stars', 0)} stars\")\n",
        "        print(f\"   üîó {repo['github_url']}\")\n",
        "        print(f\"   üìù {repo.get('summary', repo.get('description', 'No description available'))}\")\n",
        "        print()\n",
        "    \n",
        "    # Create final newsletter data structure\n",
        "    newsletter_data = {\n",
        "        \"metadata\": {\n",
        "            \"title\": \"Trending GitHub Repositories\",\n",
        "            \"subtitle\": \"Top repositories from the past 24 hours\",\n",
        "            \"generated_at\": datetime.now().isoformat(),\n",
        "            \"ranking_source\": ranking_source,\n",
        "            \"total_repositories\": len(newsletter_repos)\n",
        "        },\n",
        "        \"repositories\": newsletter_repos\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Newsletter content generated successfully!\")\n",
        "    print(f\"   Ranking source: {ranking_source}\")\n",
        "    print(f\"   Total repositories: {len(newsletter_repos)}\")\n",
        "    print(f\"   Successful descriptions: {len([r for r in newsletter_repos if r.get('status') == 'success'])}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No repository data available for description generation\")\n",
        "    newsletter_data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì∞ FINAL NEWSLETTER OUTPUT\n",
            "============================================================\n",
            "üìã Trending GitHub Repositories\n",
            "üìÖ Generated: 2025-09-29T00:11:16.807203\n",
            "üéØ Ranking: LLM\n",
            "üìä Repositories: 1\n",
            "\n",
            "1. **github/copilot-cli**\n",
            "   üåü 224 stars | üíª \n",
            "   üîó https://github.com/github/copilot-cli\n",
            "   üìù The GitHub repository for Copilot CLI provides a command-line interface to leverage Copilot's coding capabilities directly in the terminal.\n",
            "\n",
            "üîß Integration Data Structure:\n",
            "----------------------------------------\n",
            "newsletter_data = {\n",
            "    'metadata': { ... },\n",
            "    'repositories': [\n",
            "        {\n",
            "            'rank': 1,\n",
            "            'repo_name': 'github/copilot-cli',\n",
            "            'github_url': 'https://github.com/github/copilot-cli',\n",
            "            'summary': 'The GitHub repository for Copilot CLI provides a command-line interface to leverage Copilot's coding capabilities directly in the terminal.',\n",
            "            'status': 'success'\n",
            "        },\n",
            "    ]\n",
            "}\n",
            "\n",
            "‚úÖ Newsletter pipeline complete!\n",
            "üéØ Ready for integration with your main application\n",
            "\n",
            "üìä Pipeline Summary:\n",
            "   ‚úÖ GitHub trending data collected: 10 repos\n",
            "   ‚úÖ English filtering applied: 87 English repos\n",
            "   ‚úÖ README files fetched: 9 repos\n",
            "   ‚úÖ LLM ranking: Success\n",
            "   ‚úÖ Newsletter descriptions: Generated\n"
          ]
        }
      ],
      "source": [
        "# Final Newsletter Output Summary\n",
        "if 'newsletter_data' in locals() and newsletter_data:\n",
        "    print(\"üì∞ FINAL NEWSLETTER OUTPUT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"üìã {newsletter_data['metadata']['title']}\")\n",
        "    print(f\"üìÖ Generated: {newsletter_data['metadata']['generated_at']}\")\n",
        "    print(f\"üéØ Ranking: {newsletter_data['metadata']['ranking_source']}\")\n",
        "    print(f\"üìä Repositories: {newsletter_data['metadata']['total_repositories']}\")\n",
        "    print()\n",
        "    \n",
        "    for repo in newsletter_data['repositories']:\n",
        "        print(f\"{repo['rank']}. **{repo['repo_name']}**\")\n",
        "        print(f\"   üåü {repo.get('stars', 0)} stars | üíª {repo.get('primary_language', 'Unknown')}\")\n",
        "        print(f\"   üîó {repo['github_url']}\")\n",
        "        print(f\"   üìù {repo.get('summary', repo.get('description', 'No description available'))}\")\n",
        "        print()\n",
        "    \n",
        "    # Show JSON structure for integration\n",
        "    print(\"üîß Integration Data Structure:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"newsletter_data = {\")\n",
        "    print(\"    'metadata': { ... },\")\n",
        "    print(\"    'repositories': [\")\n",
        "    for repo in newsletter_data['repositories']:\n",
        "        print(f\"        {{\")\n",
        "        print(f\"            'rank': {repo['rank']},\")\n",
        "        print(f\"            'repo_name': '{repo['repo_name']}',\")\n",
        "        print(f\"            'github_url': '{repo['github_url']}',\")\n",
        "        print(f\"            'summary': '{repo.get('summary', repo.get('description', 'No description available'))}',\")\n",
        "        print(f\"            'status': '{repo.get('status', 'unknown')}'\")\n",
        "        print(f\"        }},\")\n",
        "    print(\"    ]\")\n",
        "    print(\"}\")\n",
        "    \n",
        "    print(f\"\\n‚úÖ Newsletter pipeline complete!\")\n",
        "    print(f\"üéØ Ready for integration with your main application\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå Newsletter data not available\")\n",
        "    print(\"üí° Run the previous cells to generate newsletter content\")\n",
        "\n",
        "print(f\"\\nüìä Pipeline Summary:\")\n",
        "print(f\"   ‚úÖ GitHub trending data collected: {len(repo_data_for_llm) if 'repo_data_for_llm' in locals() else 0} repos\")\n",
        "print(f\"   ‚úÖ English filtering applied: {len(df) if 'df' in locals() else 0} English repos\")\n",
        "print(f\"   ‚úÖ README files fetched: {len([r for r in df_with_readme.iterrows() if r[1]['status'] == 'success']) if 'df_with_readme' in locals() else 0} repos\")\n",
        "print(f\"   {'‚úÖ' if 'llm_ranking_data' in locals() and llm_ranking_data else '‚ùå'} LLM ranking: {'Success' if 'llm_ranking_data' in locals() and llm_ranking_data else 'Failed/Not used'}\")\n",
        "print(f\"   {'‚úÖ' if 'newsletter_data' in locals() and newsletter_data else '‚ùå'} Newsletter descriptions: {'Generated' if 'newsletter_data' in locals() and newsletter_data else 'Not generated'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
