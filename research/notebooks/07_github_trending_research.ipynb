{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GitHub Trending Projects Research\n",
        "\n",
        "This notebook explores the OSS Insight API to gather trending GitHub projects from the past 24 hours.\n",
        "\n",
        "## Objectives\n",
        "- Test the OSS Insight API for trending repositories\n",
        "- Gather data from past 24 hours (all languages)\n",
        "- Analyze the data structure and quality\n",
        "- Explore potential integration with our newsletter pipeline\n",
        "\n",
        "## API Endpoint\n",
        "- **Base URL**: `https://api.ossinsight.io/v1/trends/repos/`\n",
        "- **Parameters**: \n",
        "  - `period=past_24_hours`\n",
        "  - `language=All` (default, all languages)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Set up display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîó API URL: https://api.ossinsight.io/v1/trends/repos/?period=past_24_hours&language=All\n",
            "üìÖ Period: past_24_hours\n",
            "üåê Language: All\n",
            "‚è∞ Request time: 2025-09-27 20:34:39\n"
          ]
        }
      ],
      "source": [
        "# API Configuration\n",
        "API_BASE_URL = \"https://api.ossinsight.io/v1/trends/repos/\"\n",
        "PERIOD = \"past_24_hours\"\n",
        "LANGUAGE = \"All\"  # All languages\n",
        "\n",
        "# Build the request URL\n",
        "url = f\"{API_BASE_URL}?period={PERIOD}&language={LANGUAGE}\"\n",
        "\n",
        "print(f\"üîó API URL: {url}\")\n",
        "print(f\"üìÖ Period: {PERIOD}\")\n",
        "print(f\"üåê Language: {LANGUAGE}\")\n",
        "print(f\"‚è∞ Request time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Making API request...\n",
            "‚úÖ Request successful! (0.52s)\n",
            "üìä Status Code: 200\n",
            "üìè Response Size: 36,457 bytes\n"
          ]
        }
      ],
      "source": [
        "# Make the API request\n",
        "print(\"üöÄ Making API request...\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    response = requests.get(url, headers={'Accept': 'application/json'}, timeout=30)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    \n",
        "    request_time = time.time() - start_time\n",
        "    print(f\"‚úÖ Request successful! ({request_time:.2f}s)\")\n",
        "    print(f\"üìä Status Code: {response.status_code}\")\n",
        "    print(f\"üìè Response Size: {len(response.content):,} bytes\")\n",
        "    \n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"‚ùå Request failed: {e}\")\n",
        "    response = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ JSON parsing successful!\n",
            "\n",
            "üìã Response Structure:\n",
            "   Type: sql_endpoint\n",
            "   Data keys: ['columns', 'rows', 'result']\n",
            "   Number of repositories: 100\n",
            "   Number of columns: 11\n",
            "   Columns: ['repo_id', 'repo_name', 'primary_language', 'description', 'stars', 'forks', 'pull_requests', 'pushes', 'total_score', 'contributor_logins', 'collection_names']\n"
          ]
        }
      ],
      "source": [
        "# Parse and examine the response\n",
        "if response:\n",
        "    try:\n",
        "        data = response.json()\n",
        "        print(\"‚úÖ JSON parsing successful!\")\n",
        "        \n",
        "        # Examine the structure\n",
        "        print(f\"\\nüìã Response Structure:\")\n",
        "        print(f\"   Type: {data.get('type', 'Unknown')}\")\n",
        "        \n",
        "        if 'data' in data:\n",
        "            data_section = data['data']\n",
        "            print(f\"   Data keys: {list(data_section.keys())}\")\n",
        "            \n",
        "            # Check if we have rows\n",
        "            if 'rows' in data_section:\n",
        "                rows = data_section['rows']\n",
        "                print(f\"   Number of repositories: {len(rows)}\")\n",
        "                \n",
        "                # Show column information\n",
        "                if 'columns' in data_section:\n",
        "                    columns = data_section['columns']\n",
        "                    print(f\"   Number of columns: {len(columns)}\")\n",
        "                    print(f\"   Columns: {[col['col'] for col in columns]}\")\n",
        "        \n",
        "        # Show result metadata\n",
        "        if 'result' in data:\n",
        "            result = data['result']\n",
        "            print(f\"\\nüìä Query Result:\")\n",
        "            print(f\"   Code: {result.get('code', 'N/A')}\")\n",
        "            print(f\"   Message: {result.get('message', 'N/A')}\")\n",
        "            print(f\"   Latency: {result.get('latency', 'N/A')}\")\n",
        "            print(f\"   Row Count: {result.get('row_count', 'N/A')}\")\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå JSON parsing failed: {e}\")\n",
        "        data = None\n",
        "else:\n",
        "    print(\"‚ùå No response to parse\")\n",
        "    data = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä DataFrame created with 100 rows and 11 columns\n",
            "üìã Columns: ['repo_id', 'repo_name', 'primary_language', 'description', 'stars', 'forks', 'pull_requests', 'pushes', 'total_score', 'contributor_logins', 'collection_names']\n",
            "\n",
            "üîç First 5 repositories:\n",
            "      repo_id            repo_name primary_language  \\\n",
            "0  1042367133      github/spec-kit           Python   \n",
            "1   997220241   HKUDS/RAG-Anything           Python   \n",
            "2  1006414368  OpenCut-app/OpenCut       TypeScript   \n",
            "3   994093166     basecamp/omarchy            Shell   \n",
            "4  1062234789    JerryZLiu/Dayflow            Swift   \n",
            "\n",
            "                                                      description stars forks  \\\n",
            "0  üí´ Toolkit to help you get started with Spec-Driven Development   277    25   \n",
            "1                        \"RAG-Anything: All-in-One RAG Framework\"   246    14   \n",
            "2                              The open-source CapCut alternative   271    28   \n",
            "3                                 Opinionated Arch/Hyprland Setup   243    13   \n",
            "4                  Generate a timeline of your day, automatically   219     8   \n",
            "\n",
            "  pull_requests pushes total_score  \\\n",
            "0             1          1221.7526   \n",
            "1                        1103.4763   \n",
            "2                        1046.9045   \n",
            "3             8      1   1028.5203   \n",
            "4             1           986.5417   \n",
            "\n",
            "                                    contributor_logins collection_names  \n",
            "0    Copilot,kumaresra,qduc,odysseyalive,RoboCreator85                   \n",
            "1                                                                        \n",
            "2                                     AANDDARK,Gian511                   \n",
            "3  Somnius,iamobservable,woopstar,nextgeniuspro,rublev                   \n",
            "4                          matsfinsas,w568w,chenrui333                   \n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame for easier analysis\n",
        "if data and 'data' in data and 'rows' in data['data']:\n",
        "    rows = data['data']['rows']\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "    \n",
        "    print(f\"üìä DataFrame created with {len(df)} rows and {len(df.columns)} columns\")\n",
        "    print(f\"üìã Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Show first few rows\n",
        "    print(f\"\\nüîç First 5 repositories:\")\n",
        "    print(df.head())\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No data available to create DataFrame\")\n",
        "    df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìà Data Analysis:\n",
            "   Total repositories: 100\n",
            "\n",
            "üîç Missing Values:\n",
            "\n",
            "üåê Programming Languages Distribution:\n",
            "   Python: 31 (31.0%)\n",
            "   TypeScript: 26 (26.0%)\n",
            "   Jupyter Notebook: 8 (8.0%)\n",
            "   JavaScript: 7 (7.0%)\n",
            "   : 5 (5.0%)\n",
            "   Swift: 4 (4.0%)\n",
            "   Rust: 3 (3.0%)\n",
            "   HTML: 3 (3.0%)\n",
            "   Shell: 2 (2.0%)\n",
            "   PHP: 2 (2.0%)\n",
            "\n",
            "‚≠ê Stars Statistics:\n",
            "   Min: 6\n",
            "   Max: 277\n",
            "   Mean: 70.5\n",
            "   Median: 46.5\n",
            "\n",
            "üèÜ Top 10 Repositories by Stars:\n",
            "   github/spec-kit (Python) - 277 stars\n",
            "      üí´ Toolkit to help you get started with Spec-Driven Developme...\n",
            "\n",
            "   OpenCut-app/OpenCut (TypeScript) - 271 stars\n",
            "      The open-source CapCut alternative\n",
            "\n",
            "   HKUDS/RAG-Anything (Python) - 246 stars\n",
            "      \"RAG-Anything: All-in-One RAG Framework\"\n",
            "\n",
            "   basecamp/omarchy (Shell) - 243 stars\n",
            "      Opinionated Arch/Hyprland Setup\n",
            "\n",
            "   JerryZLiu/Dayflow (Swift) - 219 stars\n",
            "      Generate a timeline of your day, automatically\n",
            "\n",
            "   imputnet/helium (Python) - 171 stars\n",
            "      Private, fast, and honest web browser\n",
            "\n",
            "   apple/ml-simplefold (Python) - 170 stars\n",
            "      \n",
            "\n",
            "   PicoTrex/Awesome-Nano-Banana-images () - 170 stars\n",
            "      A curated collection of fun and creative examples generated ...\n",
            "\n",
            "   dataease/SQLBot (Python) - 155 stars\n",
            "      üî• Âü∫‰∫éÂ§ßÊ®°ÂûãÂíå RAG ÁöÑÊô∫ËÉΩÈóÆÊï∞Á≥ªÁªü„ÄÇText-to-SQL Generation via LLMs using R...\n",
            "\n",
            "   iChochy/NCE (CSS) - 154 stars\n",
            "      „ÄäÊñ∞Ê¶ÇÂøµËã±ËØ≠„ÄãÂÖ®ÂõõÂÜåÂú®Á∫øËØæÊñáÊúóËØª„ÄÅÂçïÂè•ÁÇπËØª\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Analyze the data quality and content\n",
        "if df is not None:\n",
        "    print(\"üìà Data Analysis:\")\n",
        "    print(f\"   Total repositories: {len(df)}\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nüîç Missing Values:\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    for col, missing in missing_values.items():\n",
        "        if missing > 0:\n",
        "            print(f\"   {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Analyze primary languages\n",
        "    if 'primary_language' in df.columns:\n",
        "        print(f\"\\nüåê Programming Languages Distribution:\")\n",
        "        lang_counts = df['primary_language'].value_counts().head(10)\n",
        "        for lang, count in lang_counts.items():\n",
        "            print(f\"   {lang}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Analyze stars distribution\n",
        "    if 'stars' in df.columns:\n",
        "        df['stars_numeric'] = pd.to_numeric(df['stars'], errors='coerce')\n",
        "        print(f\"\\n‚≠ê Stars Statistics:\")\n",
        "        print(f\"   Min: {df['stars_numeric'].min()}\")\n",
        "        print(f\"   Max: {df['stars_numeric'].max()}\")\n",
        "        print(f\"   Mean: {df['stars_numeric'].mean():.1f}\")\n",
        "        print(f\"   Median: {df['stars_numeric'].median():.1f}\")\n",
        "    \n",
        "    # Show top repositories by stars\n",
        "    if 'stars_numeric' in df.columns:\n",
        "        print(f\"\\nüèÜ Top 10 Repositories by Stars:\")\n",
        "        top_repos = df.nlargest(10, 'stars_numeric')[['repo_name', 'primary_language', 'stars', 'description']]\n",
        "        for idx, row in top_repos.iterrows():\n",
        "            desc = row['description'][:60] + \"...\" if len(str(row['description'])) > 60 else row['description']\n",
        "            print(f\"   {row['repo_name']} ({row['primary_language']}) - {row['stars']} stars\")\n",
        "            print(f\"      {desc}\")\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Failed to save data: [Errno 2] No such file or directory: '../../data/github_trending_20250927_203439.json'\n",
            "‚ùå Failed to save CSV: Cannot save file into a non-existent directory: '../../data'\n"
          ]
        }
      ],
      "source": [
        "# Save the raw data for further analysis\n",
        "if data:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"../../data/github_trending_{timestamp}.json\"\n",
        "    \n",
        "    try:\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"üíæ Raw data saved to: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save data: {e}\")\n",
        "\n",
        "# Save DataFrame as CSV for easy analysis\n",
        "if df is not None:\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    csv_filename = f\"../../data/github_trending_{timestamp}.csv\"\n",
        "    \n",
        "    try:\n",
        "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "        print(f\"üíæ DataFrame saved to: {csv_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to save CSV: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "### What We've Discovered:\n",
        "- ‚úÖ Successfully connected to OSS Insight API\n",
        "- ‚úÖ Retrieved trending repositories from past 24 hours\n",
        "- ‚úÖ Analyzed data structure and quality\n",
        "- ‚úÖ Saved data for further analysis\n",
        "\n",
        "### Potential Integration with Newsletter:\n",
        "1. **Content Source**: Trending GitHub projects could be a valuable addition to tech newsletters\n",
        "2. **Data Quality**: High-quality data with stars, descriptions, and metadata\n",
        "3. **Real-time**: Past 24 hours data provides fresh, relevant content\n",
        "4. **Diverse Languages**: Covers projects in multiple programming languages\n",
        "\n",
        "### Next Research Areas:\n",
        "- Filter for tech-relevant projects (AI, web development, etc.)\n",
        "- Analyze project descriptions for quality scoring\n",
        "- Explore integration with existing pipeline\n",
        "- Test different time periods and language filters\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
